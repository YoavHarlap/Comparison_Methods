\documentclass[12pt, a4paper, twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\setcounter{secnumdepth}{3}
\numberwithin{equation}{chapter}
\numberwithin{figure}{chapter}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage[a4paper, inner=3cm]{geometry}
\usepackage{color, soul}
\usepackage[super]{nth}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{color}
\usepackage{subcaption}

\usepackage{mathtools}

\usepackage{booktabs}
\usepackage[svgnames,table]{xcolor}
\usepackage[tableposition=above]{caption}
\usepackage{pifont}

\newcommand{\rev}[1]{{{#1}}}
\newcommand{\re}[1]{{{#1}}}

\usepackage[hang, flushmargin]{footmisc}

\usepackage[section]{placeins}
\usepackage{float}
\usepackage{setspace}
\DeclareMathOperator{\Unif}{Unif}
\usepackage{tikz}


\usetikzlibrary{backgrounds}
\makeatletter

\tikzset{%
  fancy quotes/.style={
    text width=\fq@width pt,
    align=justify,
    inner sep=1em,
    anchor=north west,
    minimum width=\linewidth,
  },
  fancy quotes width/.initial={.8\linewidth},
  fancy quotes marks/.style={
    scale=8,
    text=white,
    inner sep=0pt,
  },
  fancy quotes opening/.style={
    fancy quotes marks,
  },
  fancy quotes closing/.style={
    fancy quotes marks,
  },
  fancy quotes background/.style={
    show background rectangle,
    inner frame xsep=0pt,
    background rectangle/.style={
      fill=gray!25,
      rounded corners,
    },
  }
}

\newenvironment{fancyquotes}[1][]{%
\noindent
\tikzpicture[fancy quotes background]
\node[fancy quotes opening,anchor=north west] (fq@ul) at (0,0) {``};
\tikz@scan@one@point\pgfutil@firstofone(fq@ul.east)
\pgfmathsetmacro{\fq@width}{\linewidth - 2*\pgf@x}
\node[fancy quotes,#1] (fq@txt) at (fq@ul.north west) \bgroup}
{\egroup;
\node[overlay,fancy quotes closing,anchor=east] at (fq@txt.south east) {''};
\endtikzpicture}

\makeatother

\newcommand*{\BeginNoToc}{%
  \addtocontents{toc}{%
    \edef\protect\SavedTocDepth{\protect\the\protect\value{tocdepth}}%
  }%
  \addtocontents{toc}{%
    \protect\setcounter{tocdepth}{-10}%
  }%
}
\newcommand*{\EndNoToc}{%
  \addtocontents{toc}{%
    \protect\setcounter{tocdepth}{\protect\SavedTocDepth}%
  }%
}

%\usepackage[Lenny]{fncychap}

\raggedbottom
%\DeclareMathOperator{\IDFT}{IDFT}

\usepackage[nottoc]{tocbibind}
\usepackage{fancyhdr}

\pagestyle{fancy}
\newcommand{\fncyfront}{%
\fancyhead[RO]{{\footnotesize \rightmark }}
\fancyfoot[RO]{\thepage }
\fancyhead[LE]{\footnotesize {\leftmark }}
\fancyfoot[LE]{\thepage }
\fancyhead[RE, LO]{}
\fancyfoot[C]{}
\renewcommand{\headrulewidth}{0.3 pt}}
\newcommand{\fncymain}{%
\fancyhead[RO]{{\footnotesize \rightmark}}
\fancyfoot[RO]{\thepage }
\fancyhead[LE]{{\footnotesize \leftmark}}
\fancyfoot[LE]{\thepage }
\fancyfoot[C]{}
\renewcommand{\headrulewidth}{0.3 pt}}

\pagestyle{empty}
\newenvironment{abstract}%
{\cleardoublepage \null \vfill \begin{center}%
\bfseries \abstractname \end{center}}%
{\vfill \null}

\usepackage{sectsty}
\allsectionsfont{\sffamily}

%\usepackage{appendix}

\usepackage[titletoc]{appendix}

\usepackage{natbib}
\usepackage{graphicx}

\usepackage{amsthm}
%\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\usepackage{algorithm}
\usepackage{algorithmicx}

%\usepackage{pdfpages}

\usepackage{hyperref}

\hypersetup{
	colorlinks,
	citecolor=blue,
	filecolor=black,
	linkcolor=blue,
	urlcolor=blue
}

%opening
\title{Comparative Analysis of Phase Retrieval and Matrix Completion}
\author{Yoav Harlap\\ Under the supervision of Prof. Tamir Bendory}

\begin{document}
\pagestyle{fancy}
\fncyfront
\frontmatter
\maketitle
\begin{abstract}
This thesis considers the phase retrieval and matrix completion problems, both aiming to reconstruct vectors and matrices from incomplete or corrupted observations, with a focus on leveraging iterative and projection-based algorithms. We are interested in scenarios where some additional information about the objects being reconstructed can be assumed. Inspired by the challenge of structural reconstruction, our goal is to determine the global phase vector from the Fourier magnitude data collected by the sensor. We will discuss matrix completion with a focus on low-rank recovery and explore interesting applications. We will present different algorithms that solve these difficult problems. We will evaluate the performance of each one, including its advantages and disadvantages in various cases. We will provide many numerical results and conclusions to offer insight and conceptual value to those who would like to solve similar challenges. The code is available at: ~\url{https://github.com/YoavHarlap/Comparison_Methods}.
\end{abstract}
\chapter*{Acknowledgments}
123456789.
{
\singlespacing
  \hypersetup{linkcolor=black}
  \tableofcontents
\BeginNoToc
\newpage
\listoffigures
%\newpage
%\listoftables
\EndNoToc
}
\onehalfspacing
\fncymain
\mainmatter
\chapter{Introduction}
\label{ch:intro}
\section{Overview of Phase Retrieval}

Phase retrieval is a fundamental problem in various fields such as optics ~\cite{fienup1982phase, shechtman2015phase}, signal processing ~\cite{candes2013phaselift, balan2006signal}, crystallography ~\cite{millane1990phase, hauptman1986minimal}, and quantum mechanics ~\cite{goy2018experimental, gross2010quantum}. It involves reconstructing a signal or image from the magnitude of its Fourier transform, where the phase information is missing or inaccessible. Since many physical measurement devices can only capture intensity (which is proportional to the magnitude squared of a signal) and not the phase, phase retrieval becomes essential for accurate reconstruction of the original signal or image.

The simulation in \ref{fig:phase_transition}  demonstrates how changes in phase affect the image, leading to a gradual improvement in clarity as the phase shifts from random to correct phase values, while the magnitude remains the same. Each frame in the image grid represents a 20\% increase in phase alignment until it reaches the original image.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures1/Phase Transition from Random to Correct Phase.png}
    \caption[Phase Transition from Random to Correct Phase]{Simulation showing phase impact on image clarity as phase shifts from random to correct phase values, with constant magnitude. Each frame represents a 20\% increase in phase alignment until it reaches the correct phase values in the original image.}
    \label{fig:phase_transition}
\end{figure}



\section{Problem Definition: Rigorous Approach}
We consider a matrix \mbox{$A \in \mathbb{C}^{m\times n}$}, which will be referred to as the sensing matrix and a magnitude vector \mbox{$b \in \mathbb{R}^{m}$}, with the understanding that b elements are non-negative. Our task in the phase retrieval problem is to solve the following system of equations successfully:
\begin{equation}
\label{eq:model}
|Ax_0| = b
\end{equation}
\begin{itemize}
    \item The absolute value is applied to each element of the vector \( v \) individually, such that \[|v| = (|v_1|, |v_2|, \dots, |v_n|)\] where \( |v_i| \) represents the absolute value of each entry in \( v \).

    \item The matrix \( A \) can serve various purposes. Often, \( A \) is the Discrete Fourier Transform (DFT) matrix, where multiplying a vector \( x \) with \( A \) performs the Fourier transform. In this case, we are interested in the scenario where \( m = n \). We can also consider cases where \( A \) is a real random matrix or a complex random matrix, with \( m > n \) in the random scenarios.
\end{itemize}
In many problems in the world, as well as in our problem, we can assume certain things about our solution, which we refer to as additional information. This additional information consists of the constraints that our solution must meet.

In addition, we can also define our problem in the following way, and the reason for doing so will become clearer in the next chapter (see Chapter \ref{chap:projectionSetsMethod}). We can look for a point \( x \in \mathcal{A} \cap \mathcal{B} \), where \( \mathcal{B} \) is the set of all signals that satisfy Equation \ref{eq:model} and is defined as:
\begin{equation}
\mathcal{B} = \left\{ y \in \mathbb{C}^m : |y| = b \right\}
\label{eq:model2}
\end{equation}
The set \( \mathcal{A} \) typically represents additional constraints known about our solution, such as sparsity or finite support.



% \section{Historical Context and Applications}

% Phase retrieval has a significant history, especially in X-ray crystallography. As noted by Walther~\cite{walther1963synthesis}, in the past, scientists faced the challenge of determining the structure of crystalline materials using X-ray diffraction patterns, which provide only intensity information. This missing phase information, known as the "phase problem," was a major obstacle to accurately determining atomic structures. while the magnitude gives us the energy distribution, the phase reveals the actual structure. The phase retrieval problem extends beyond filling in missing data; it’s about unlocking the complete potential of our measurements to understand the true nature of the object being studied.

% Over the years, phase retrieval has been crucial in many fields. In optics, it’s used for designing systems and reconstructing images from diffraction patterns~\cite{born1999principles}. In astronomy, it helps improve image resolution by correcting for atmospheric distortions. With advances like X-ray free-electron lasers and electron microscopy, phase retrieval remains vital.

\section{Challenges and Limitations in Phase Retrieval Algorithms}
Over the past decade, considerable scholarly focus has been directed toward the computational and theoretical dimensions of the phase retrieval problem. To streamline the intricacies of mathematical analysis, a trend emerged wherein researchers adopted an idealized model—one abstracted from practical applications—where the entries of \(A\) are independently and identically distributed (i.i.d.) according to a normal distribution or other similar statistical frameworks. We shall henceforth designate the endeavor of reconstructing a signal under these conditions as the *random phase retrieval problem*. Among the most prevalent algorithms for addressing this problem are those that rely on the minimization of non-convex loss functions, such as non-convex least squares, through the application of first-order gradient methods (e.g., see [14, 15, 46, 13]). Importantly, this body of work has yielded robust theoretical assurances, demonstrating that the non-convex nature of the problem is typically well-behaved when the number of measurements \(m\) significantly exceeds the dimensionality \(n\) of the signal being recovered [44, 16].

Regrettably, it has become evident that the random phase retrieval problem presents significantly less complexity compared to the authentic phase retrieval problem, particularly when \(A\) is a matrix of the Fourier-type. For most practical phase retrieval applications, algorithms developed for the random phase retrieval framework prove inadequate: the inherent non-convexity of the problem does not exhibit benign behavior, and gradient-based methods often converge to local minima, far from any global solution (see the detailed analysis in [25]). Consequently, despite extensive contributions to this literature, these methods have wielded only marginal impact on real-world applications. In practice, heuristic algorithms dominate, including the hybrid input-output (HIO) [27], difference map [22], relaxed averaged alternating reflections (RAAR) [35], and relaxed reflect reflect (RRR) [23]. Each of these methods may be viewed as an extension of the Douglas-Rachford algorithm [19]; an introduction to these algorithms is provided in Section 3. By employing a slight terminological generalization, we shall refer to these approaches as Douglas-Rachford type algorithms. While these algorithms demonstrate strong empirical performance, their theoretical properties when applied to the non-convex phase retrieval problem remain largely elusive.

Key challenges include:
\begin{itemize}
    \item \textbf{Non-Uniqueness:} Different signals can share the same Fourier magnitude, leading to ambiguity ~\cite{Candes_2015}.
    \item \textbf{Noise Sensitivity:} Noise in measurements can significantly affect the reconstruction quality~\cite{elser2018benchmarkproblemsphaseretrieval}.
    \item \textbf{Computational Complexity:} Solving phase retrieval problems often requires complex, computationally demanding algorithms~\cite{Li_2017}.
     \item \textbf{Non-Convexity:} The solution space is non-convex, making optimization methods prone to local minima~\cite{Sun_2017,Chen_2019}. Both the alternating projection technique and gradient-based methods often struggle to yield meaningful solutions; they tend to converge rapidly to suboptimal local minima, rather than reaching a solution point. In practice, a set of algorithms that can be viewed as extensions of the Douglas-Rachford scheme is commonly used.
\end{itemize}



% \section{Intuitive Understanding and Challenges in Phase Retrieval}

% The phase retrieval problem can be understood intuitively as follows: Given the magnitude of the Fourier transform of a signal, how do we recover the signal? The challenge arises because the Fourier phase is lost during measurement. Thus, reconstructing the signal involves dealing with incomplete information.

% % Key challenges include:
% % \begin{itemize}
% %     \item \textbf{Non-Uniqueness:} Different signals can share the same Fourier magnitude, leading to ambiguity ~\cite{Candes_2015}.
% %     \item \textbf{Noise Sensitivity:} Noise in measurements can significantly affect the reconstruction quality~\cite{elser2018benchmarkproblemsphaseretrieval}.
% %     \item \textbf{Computational Complexity:} Solving phase retrieval problems often requires complex, computationally demanding algorithms~\cite{Li_2017}.
% %      \item \textbf{Non-Convexity:} The solution space is non-convex, making optimization methods prone to local minima~\cite{Sun_2017,Chen_2019}. Both the alternating projection technique and gradient-based methods often struggle to yield meaningful solutions; they tend to converge rapidly to suboptimal local minima, rather than reaching a solution point. In practice, a set of algorithms that can be viewed as extensions of the Douglas-Rachford scheme is commonly used.
% % \end{itemize}

\section{Research Scope and Objectives}

This thesis focuses on evaluating the performance of existing algorithms in various scenarios. The research aims to compare the effectiveness of these algorithms in different contexts:
\begin{itemize}
    \item \textbf{Phase retrieval with unknown phase information:} Comparing how algorithms perform when the phase is unknown and only the magnitude of the Fourier transform is provided.
    \item \textbf{Phase Retrieval with Random Matrices:} Assessing algorithm performance when the matrix $A$ is random rather than a Fourier transform. This encompasses practical scenarios involving real-valued scenes and complex-valued composite scenes. Recently, it has become interesting since in order to solve the problem with a Fourier matrix, many researchers have turned to solving the problem with random matrices which is easier and it will be interesting to compare performance.
    \item \textbf{Matrix Completion:} ============= connect to phase3333 33333 Investigating algorithms for matrix completion when some matrix elements are missing. This involves using knowledge of the matrix rank and initial clues (i.e., entries that are not missing) to recover the missing elements.
\end{itemize}
By evaluating these scenarios, the thesis aims to provide insights into the strengths and limitations of existing algorithms under various conditions, thereby offering practical guidance for their application in real-world problems. The thesis will also examine the use of various heuristic algorithms in practice, such as the Hybrid Input-Output (HIO) \cite{fienup1982phase}, Relaxed Averaged Alternating Reflections (RAAR) \cite{luke2004relaxed}, and Relaxed Reflect Reflect (RRR) \cite{elser2017matrix}. These techniques, often seen as extensions of the Douglas-Rachford algorithm \cite{douglas1956numerical}, are collectively referred to as Douglas-Rachford type algorithms. While these algorithms demonstrate strong empirical performance, their properties in the context of the non-convex phase retrieval problem remain largely unexplored.


\section{Structure of the Thesis}



The thesis is structured as follows:
\begin{itemize}
    \item \textbf{Chapter 2:} Covers the mathematical foundations of phase retrieval and matrix completion, defining the problems and discussing their connection.

    \item \textbf{Chapter 3:} Presents algorithms for phase retrieval and matrix completion, accompanied by visual examples to illustrate their application.
    \item \textbf{Chapter 4:} ---
    \item \textbf{Chapter 5:} ---
    \item \textbf{Chapter 6:} Concludes the thesis with a summary of findings, contributions, and recommendations for future research.
\end{itemize}

\chapter{Mathematical Formulation and Explanations}
\section{Problem Definition: Rigorous Approach}
We consider a matrix \mbox{$A \in \mathbb{C}^{mxn}$}, which will be referred to as the sensing matrix and a magnitude vector \mbox{$b \in \mathbb{R}^{m}$}, with the understanding that b elements are non-negative. Our task in the phase retrieval problem is to solve the following system of equations successfully:
\begin{equation}
\label{eq:model}
|Ax_0| = b
\end{equation}
\begin{itemize}
    \item The absolute value is applied entry-wise: \( |v| \) where \( |v_{i}| \) represents the absolute value of each entry in the vector \( v \).

    \item The matrix \( A \) can serve various purposes. Often, \( A \) is the Discrete Fourier Transform (DFT) matrix, where multiplying a vector \( x \) with \( A \) performs the Fourier transform. In this case, we are interested in the scenario where \( m = n \). We can also consider cases where \( A \) is a real random matrix or a complex random matrix, with \( m \gg n \) in the random scenarios.
\end{itemize}
In many problems in the world, as well as in our problem, we can assume certain things about our solution, which we refer to as additional information. This additional information consists of the constraints that our solution must meet.

In addition, we can also define our problem in the following way, and the reason for doing so will become clearer in the next chapter (see Chapter \ref{chap:projectionSetsMethod}). We can look for a point \( x \in \mathcal{A} \cap \mathcal{B} \), where \( \mathcal{B} \) is the set of all signals that satisfy Equation \ref{eq:model} and is defined as:
\begin{equation}
\mathcal{B} = \left\{ y \in \mathbb{C}^m : |y| = b \right\}
\label{eq:model2}
\end{equation}
The set \( \mathcal{A} \) typically represents additional constraints known about our solution, such as sparsity or finite support which will be better understood later.

\section{Matrix Completion}
Matrix completion is a method used to reconstruct a matrix from a subset of its entries. It is widely applied in fields like signal processing, recommendation systems, and image processing.

The foundational approach to matrix completion involves low-rank matrix recovery, initially addressed by Candes and Recht~\cite{candes2009exact} through nuclear norm minimization. This technique minimizes the nuclear norm (the sum of singular values) of the matrix, which serves as a convex approximation of the rank function.

Recent advancements in matrix completion have introduced techniques that incorporate additional structural information or constraints. These methods extend classical algorithms to address new scenarios and requirements. Additionally, integrating machine learning and adaptive techniques has shown potential for enhancing performance and generalization. One of the most common examples for matrix completion is the Sudoku puzzle shown in Figure \ref{fig:sudokuExample}.


\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures1/sudoku_1}
    \caption{Original Sudoku puzzle}
    \label{fig:sudokuOriginal}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures1/sudoku_2}
    \caption{Elements to be reconstructed (in blue)}
    \label{fig:sudokuMissing}
  \end{subfigure}
  \caption[An example of a matrix completion problem in Sudoku]{An example of a matrix completion problem is the Sudoku game. We are given the initial clues (the elements we know) and need to reconstruct the missing elements (the cells painted in blue).}
  \label{fig:sudokuExample}
\end{figure}


\section{Phase Retrieval versus Matrix Completion}
Phase retrieval and matrix completion both reconstruct data from incomplete information but face unique challenges. Phase retrieval recovers phase from magnitude data, while matrix completion fills in missing entries. Comparative analysis reveals differences in noise handling, optimization, and complexity. Key metrics include reconstruction accuracy, efficiency, and noise robustness. This thesis will build on these insights to evaluate and compare algorithms for both problems, exploring various scenarios and discussing performance factors.Our algorithms are designed to address both problems, although we will customize the projections specifically for each case.Our algorithms are designed to address both problems, although we will customize the projections specifically for each case.


\chapter{Projection on Sets Method and Algorithms}
\label{chap:projectionSetsMethod}
\section{Set Projection Example Illustrated with Sudoku}
Before presenting a rigorous discussion on the projection methods, an illustrative example using Sudoku will be provided. One of the methods to solve Sudoku is with the help of the projection method and by viewing the problem visually. Let's say that our large space is the space of all solutions for arranging numbers 1-9 in the cells of the Sudoku matrix. To solve the game, we have to obey several rules. The first rule is that we must use the initial clues we received (i.e., the numbers we already have at the beginning of the game). The second rule requires us to have the numbers 1-9 in each column, as well as in each row and block. We will say that each rule is a constraint on our solution. Each constraint defines a subspace within our large space. To solve the Sudoku, we will want to find the intersection of all these sub-spaces, which means that all our constraints are satisfied. Now, it is useful and understandable to define our problem as we have defined it in \ref{eq:model2}.

\begin{figure}[ht]
  \centering
  \resizebox{0.8\textwidth}{!}{ % Scale down the entire figure
    \begin{minipage}{\textwidth}
      \centering
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_1}
        \caption{Row constraint: Each row must contain the numbers 1-9.}
        \label{fig:sudokuRow}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_2}
        \caption{Column constraint: Each column must contain the numbers 1-9.}
        \label{fig:sudokuColumn}
      \end{subfigure}

      \vspace{0.5cm}

      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_3}
        \caption{Block constraint: Each of the 9 distinct 3x3 block must contain the numbers 1-9.}
        \label{fig:sudokuBlock}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_4}
        \caption{Solution: The completed Sudoku grid where all constraints are satisfied.}
        \label{fig:sudokuSolution}
      \end{subfigure}
    \end{minipage}
  }
  \caption[Examples of Sudoku constraints]{Examples of Sudoku constraints: row, column, block, and the solution. Each constraint requires that the numbers 1-9 be used exactly once in the respective row, column, or block.}
  \label{fig:sudokuConstraints}
\end{figure}
After understanding how to define a problem using spaces and sub-spaces, we want to find the intersection point using a computationally efficient method. More mathematically, we start with a random initial vector in our large space. We then define a subspace according to the constraint and project the vector onto this subspace. This process ensures that our vector satisfies the specific constraint. Our problem arises when we have multiple constraints (see \ref{fig:sudokuConstraints}): if we force our solution to fulfill one constraint and then impose it on the second subspace, it will, of course, satisfy the second constraint but not necessarily fulfill the first.
The naive method in our case to solve the Sudoku is to iteratively project each constraint one by one and hope it converges. That is, to project one constraint, then the second constraint, and so on, and then again the first constraint, and so on again (because the projection of the previous constraint did not preserve the first constraint). Later, we will see smarter ways to solve this using others methods of projections.
\section{Defining and Analyzing Key Algorithms for Phase Retrieval}
In the following sections, we will explore several key algorithms that are crucial for solving the phase retrieval problem. We will begin by presenting their mathematical definitions and formulations. Specifically, we will examine the Hybrid Input-Output (HIO), Relaxed Averaged Alternating Reflections (RAAR), and  Relaxed Reflect Reflect  (RRR) algorithms. After defining these algorithms, we will discuss the specific projections used in each scenario under investigation. These iterative algorithms, based on projection techniques, serve as generalizations of the Douglas-Rachford algorithm. They play a significant role in tackling the challenges of phase retrieval by iteratively refining solutions through projection-based updates. This discussion will provide deeper insights into their practical applications and effectiveness.

Let \( y \in \mathbb{C}^n \) be a vector in \( n \)-dimensional complex space. We define the projection of \( y \) onto a set \( \mathcal{A} \) as \( P_\mathcal{A}(y) \), and the projection onto a set \( \mathcal{B} \) as \( P_\mathcal{B}(y) \). We will explicitly state later what each projection is, and it also depends on the specific problem we are solving. A solution is a point where the projections onto both sets $\mathcal{A}$ and $\mathcal{B}$ coincide, meaning that the projections of the point are in the intersection $\mathcal{A} \cap \mathcal{B}$.

\begin{definition}
A point \( y_0 \in \mathbb{C}^m \) is considered a solution if it satisfies the condition \( P_{\mathcal{A}}(y_0) = P_{\mathcal{B}}(y_0) \).
\end{definition}


In practice, many times we will be satisfied with equality up to a point of tolerance, or it will be used as a requirement in the noisy case. This condition will constitute the stopping criterion for our algorithms (except where otherwise stated). Intuitively, when we refer to projections as constraints, it is clear that if the projection of the vector onto one space is the same as the projection onto another space, then this is a solution. We have found a vector that satisfies all our constraints.





\section{Algorithm Iterative Steps}
Now, we present how each algorithm updates its iterative step, with all the following algorithms beginning from a random point in the space.
The Alternating Projections method i.e. projects its constraints one after the other, which was implicitly referenced in the previous subsection, is detailed first. \noindent In Figure \ref{fig:proj_circle}, an illustration of iterations in a projection-based algorithm in a 2-dimensional space is presented. In this example, the goal was to find the intersection of three circles using orthogonal projections onto them. The process began from a random point and continued iteratively until convergence to the solution. It is important to note that there are cases where convergence to a solution may not occur quickly or, in some cases, may not occur at all. This images is referenced from the lecture by Andrew Maiden in 2022 \cite{Maiden2022}.


\begin{figure}[H]
    \centering

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_1}
        \caption{Start from a random point}
        \label{fig:proj_circle_1_1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_2}
        \caption{Projection on the first circle}
        \label{fig:proj_circle_1_2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_3}
        \caption{Projection on the second circle}
        \label{fig:proj_circle_1_3}
    \end{minipage}

    \vspace{0.5cm}

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_4}
        \caption{Projection on the third circle}
        \label{fig:proj_circle_1_4}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_5}
        \caption{First step of the algorithm}
        \label{fig:proj_circle_1_5}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_6}
        \caption{Convergence to the intersection point}
        \label{fig:proj_circle_1_6}
    \end{minipage}

    \caption[Illustration of an Alternating Projections algorithm using orthogonal projections.]{Illustration of an Alternating Projections algorithm. The figure shows iterations in a projection-based algorithm within a 2-dimensional space. The goal is to find the intersection point of three circles using orthogonal projections. The process starts from a random point and proceeds iteratively. These images are referenced from the lecture by Andrew Maiden in 2022 \cite{Maiden2022}.}
    \label{fig:proj_circle}
\end{figure}

\subsection{Alternating Projections}

The Alternating Projections method involves iteratively projecting onto different sets. The update step for this method is given by:
\begin{equation}
y^{(k+1)} = P_A \left( P_B \left( y^{(k)} \right) \right)
\end{equation}
where \( y^{(k)} \) is the vector at the \( k \)-th iteration, and \( P_A \) and \( P_B \) are the projection operators onto sets \( A \) and \( B \), respectively. This method alternates between projecting onto each set to iteratively approach a solution that satisfies all constraints. The main problem with this algorithm is that it often gets stuck at a particular point, as illustrated in Figure \ref{fig:algorithm_behaviors}.

\subsection{Hybrid Input-Output (HIO)}

The Hybrid Input-Output (HIO) algorithm updates its iterative step as follows:
\begin{equation}
y^{(k+1)} = y^{(k)} + P_A \left( (1 + \alpha)P_B(y^{(k)}) - y^{(k)} \right) - \alpha P_B \left( y^{(k)} \right)
\end{equation}
where \( \alpha \) is a parameter controlling the amount of feedback applied in the iteration process.

\subsection{Relaxed Averaged Alternating Reflections (RAAR)}

The Relaxed Averaged Alternating Reflections (RAAR) algorithm updates its iterative step as:
\begin{equation}
y^{(k+1)} = \beta   \left(y^{(k)}+ P_A \left( 2P_B(y^{(k)})- y^{(k)}\right) \right) +
(1-2\beta)  P_B \left( y^{(k)} \right)
\end{equation}

where \( \beta \) is a parameter controlling the balance between the two projections \( P_{\mathcal{A}} \) and \( P_{\mathcal{B}} \).

\subsection{ Relaxed Reflect Reflect (RRR)}

The  Relaxed Reflect Reflect (RRR) algorithm updates its iterative step as:
\begin{equation}
y^{(k+1)} = y^{(k)} +\gamma  \left( P_A \left( 2P_B(y^{(k)})- y^{(k)}\right) -  P_B \left( y^{(k)} \right) \right)
\end{equation}

where \( \gamma \) is a parameter similar to \( \beta \) in RAAR, determining the weight given to each projection. One advantage of this algorithm is that it oscillates, which helps it avoid getting stuck in a local minimum, unlike the Alternating Projections (AP) method, see figure \ref{fig:algorithm_behaviors}. This oscillatory behavior often leads to excellent performance, as will be further discussed in the next chapter.
We will consider cases where \( \alpha \), \( \beta \), and \( \gamma \) lie in the interval \((0, 1)\).


\begin{figure}[h!]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/AP_example_stuck_1}
        \caption{Alternating Projections method getting stuck.}
        \label{fig:ap_stuck}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/RRR_example_oscillate_1}
        \caption{RRR algorithm exhibiting oscillations.}
        \label{fig:rrr_oscillate}
    \end{minipage}
    \caption[RRR and Alternating Projections algorithms exhibit different behaviors.]{Illustrations of different algorithm behaviors: (left) Alternating Projections method getting stuck and (right) RRR algorithm oscillating. This is an example of running a composite random matrix \(A\) drawn i.i.d. from a normal distribution with size \(50 \times 15\). The run was stopped after 1000 iterations when the Alternating Projections (AP) algorithm still did not converge at this stage, whereas the Relaxed Reflect Reflect (RRR) algorithm had already converged.
}
    \label{fig:algorithm_behaviors}
\end{figure}

\section{Detailed Projections for Case Studies}
In this section, we provide a comprehensive overview of the specific projection operations employed for each scenario analyzed in this paper, outlining how these projections are applied across various cases to solve the phase retrieval and matrix completion problems. As we have mentioned numerous times, the solution to our problem is to find the intersection point  \( x_0 \in \mathcal{A} \cap \mathcal{B} \). To address this, we define projectors onto these sets, with the requirement that for practical algorithms, these projectors must be efficiently computable.
The main cases we will discuss are:

\begin{enumerate}
    \item The matrix \( A \) in the model defined in \(\ref{eq:model}\) is a random matrix, where every element is drawn i.i.d. from a normal distribution, and it can be either real or complex. This case is called the "random case." It is interesting, even though it is considered easier, because many researchers start with this case before addressing the next, more complex scenarios.

    \item Phase retrieval definitions, as formulated in \cite{elser2018benchmarkproblemsphaseretrieval}, are practical, particularly in the challenge of structural reconstruction in single-particle Cryo-Electron Microscopy. In this case, our matrix \( A \) is the DFT matrix, and thus we refer to this case as the "DFT case." As explained in \cite{elser2018benchmarkproblemsphaseretrieval}, we can assume that our signal is sparse, which will be relevant for one of our projections later.

    \item Matrix completion: In this case, one projector ensures that the initial elements are imposed, and the second projector usually ensures that the rank of the matrix is \( r \). This case is referred to as "Matrix Completion."
\end{enumerate}




For a general \( x \in \mathbb{C}^n \), let \( y = Ax \in \mathbb{C}^m \). We focus on projectors in terms of \( y \) rather than \( x \) because computing the projector onto \( \mathcal{B} \) is significantly less expensive \cite{Li_2017, levin2020notedouglasrachfordgradientsphase}. In the random and the DFT cases, the projector of \( y \) onto the set \( \mathcal{B} \) is defined as

\[
P_{\mathcal{B}}(y) = b \odot \text{phase}(y),
\]

where \( b \) represents the measured magnitudes (see equation \ref{eq:model}), \( \odot \) denotes the point-wise product, and the phase operator is defined element-wise as

\[
\text{phase}(y)[i] := \frac{y[i]}{|y[i]|}, \quad y[i] \neq 0,
\]

and zero otherwise.


In the random case, the entries of the sensing matrix are typically drawn i.i.d. from a normal distribution with \( m > 2n \). A point \( y_0 \in \mathbb{C}^m \) that represents a solution should lie within the column space of the matrix \( A \), which means \( y_0 = A A^\dagger y_0 \), where \( A^\dagger \) denotes the pseudo-inverse of \( A \). The column space of \( A \), also known as the image of \( A \), includes all vectors that can be expressed as \( A x \) for some vector \( x \).

Thus, for any vector \( y \in \mathbb{C}^m \) to be a valid solution, it must be in the image of \( A \), meaning there exists a vector \( x \) such that \( y = A x \). Specifically, the projection of \( y \) onto the column space of \( A \) is given by:

\[
P_A(y) = A A^\dagger y.
\]

In the DFT case, we assume the additional information discussed earlier in the paper: the signal is known to be sparse. As a result, we impose a sparsity constraint on the solution (see \cite{elser2018benchmarkproblemsphaseretrieval}), in addition to the projection on B that we mentioned earlier.

Sparsity refers to the property where most elements of a vector are zero, with only a few non-zero entries. Specifically, for a vector \( x \in \mathbb{C}^n \), we say it is \( S \)-sparse if it has at most \( |S| \) non-zero entries. In this case, the second projection imposes the assumption of sparsity by retaining the \( |S| \) highest-magnitude elements of the vector and setting the rest to zero.

The projection operator \( P_S \) onto the space of \( S \)-sparse vectors is defined as follows: for a given vector \( x \in \mathbb{C}^n \), \( P_S(x) \) is obtained by:

\[
P_S(x)[i] =
\begin{cases}
x[i], & \text{if } i \text{ corresponds to one of the } |S| \text{ largest elements of } |x|, \\
0, & \text{otherwise}.
\end{cases}
\]

This projection ensures that the sparsity constraint is enforced on the signal, keeping the \( |S| \) largest-valued pixels unchanged while setting the rest to zero. By applying this projection iteratively, we can ensure that our solution aligns with the assumed sparsity of the signal.

\textbf{Note:} We impose the sparsity constraint on our vector \( x \) rather than on vector \( y \) as before. However, it is important to highlight that in the DFT case, the matrix \( A \) is invertible (IDFT). Therefore, it does not matter whether we apply the operations to \( y \) or \( x \), as long as we remain consistent in our approach.


As extensively discussed in the article by \cite{elser2018benchmarkproblemsphaseretrieval}, we will define the stopping conditions slightly differently here.
We set our stopping condition as follows:

\[
\frac{\text{I}_\text{S}}{\text{I}_\text{F}} > 0.95
\]

where \(\text{I}_\text{F}\), the power of the entire image (the sum of each pixel squared), is defined as:

\[
\text{I}_\text{F} = \sum_{x=0}^{M-1} \sum_{y=0}^{M-1} \rho^2(x, y),
\]

and \(\text{I}_\text{S}\), the power of the pixels in the support \( S \), is defined as:

\[
\text{I}_\text{S} = \sum_{(x,y) \in S} \rho^2(x, y).
\]

where \( \rho(x, y) \) represents the values in the reconstructed image, and \( S \) represents the support of the image.


We note that increasing the threshold value above 0.95 will lead to longer convergence times, but the performance of the reconstruction will improve in terms of accuracy.


\section{Projections for Matrix Completion Case}

In the case of Matrix Completion, the projections are different from the previous cases. The first projection enforces the known entries of the matrix, ensuring that the matrix matches the given values at specific locations. This can be represented using a mask operator that preserves the known entries and leaves the unknown entries unchanged. The second projection imposes a rank constraint, forcing the matrix to have a rank of \( r \), which is assumed based on additional information.

Let \( M \in \mathbb{R}^{m \times n} \) represent the matrix we want to complete, and let \( \Omega \) denote the set of indices where the matrix entries are known. We define the mask operator \( P_{\Omega} \) as follows:

\[
P_{\Omega}(X,M)[i,j] =
\begin{cases}
M[i,j], & (i,j) \in \Omega, \\
X[i,j], & (i,j) \notin \Omega,
\end{cases}
\]
where \( X \) is the matrix before projection. This projection ensures that the given matrix elements remain unchanged, while the unknown elements can still be updated in subsequent iterations.

The second projection forces the matrix to have a rank of \( r \). This is done using the Singular Value Decomposition (SVD). Suppose the matrix \( M \) has the SVD:

\[
M = U \Sigma V^*
\]
where \( \Sigma \) is the diagonal matrix of singular values, and \( U \), \( V \) are orthogonal matrices. The projection onto the set of rank-\( r \) matrices, denoted as \( P_r(M) \), is given by:

\[
P_r(M) = U \Sigma_r V^*
\]

where \( \Sigma_r \) is obtained by retaining only the largest \( r \) singular values and setting the rest to zero. This projection ensures that the resulting matrix has rank \( r \), while keeping the structure of the matrix close to the original.

This process is analogous to frequency filtering: by imposing the rank-\( r \) constraint, we eliminate the smallest singular values (which can be seen as high-frequency components), but the overall structure of the matrix remains largely unchanged.

\section{Number of Missing Entries in Matrix Completion}
\subsection{The Netflix Matrix Model: Recommendations and Ratings Analysis}
The Netflix matrix model consists of rows representing users and columns representing movies, as detailed in Table ~\ref{table:Netflix_table}. Our objective is to recommend movies to users based on their preferences. This model incorporates data on how users rated movies, typically expressed as percentage values. Understanding how many entries need to be provided and how many can be missing is crucial for effective matrix completion. Additionally, the matrix rank r is considered, which provides insights into the underlying structure of the ratings. While examining whether this matrix rank reflects mathematical patterns—such as the tendency of similar users to enjoy similar movies or the preference of individuals for specific genres—is intriguing, it is beyond the scope of our current discussion.

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
 & \textbf{The Lion King} & \textbf{Avatar} & \textbf{Inception} & \textbf{Titanic} & \textbf{The Avengers} \\
\hline
\textbf{John} & 99\% & 85\% & 80\% & - & 75\% \\
\hline
\textbf{Emily} & - & 10\% & - & 5\% & 77\% \\
\hline
\textbf{Michael} & 99\% & 87\% & 12\% & 90\% & 80\% \\
\hline
\textbf{Sarah} & 91\% & - & 1\% & 97\% & - \\
\hline
\end{tabular}
\caption[Netflix Matrix Model]{Netflix Matrix Model, There are elements in the matrix that need to be completed based on our algorithms}
\label{table:Netflix_table}
\end{table}

\subsection{Maximum Allowable Number of Missing Elements in Matrix Completion}

In this section, we investigate the recovery of a matrix from incomplete data. Specifically, we consider the scenario where we can delete up to \(({n-r})^2\) elements and still recover the matrix in full, where \(r\) denotes the rank of the matrix. This scenario is optimal and significant because removing one more element beyond this point would make reconstruction impossible.

\begin{theorem}
Let \( \mathbf{M} \) be a matrix of rank \( r \) with dimensions \( n \times n \). If we delete up to \(({n-r})^2\) elements from \( \mathbf{M} \), the remaining elements are sufficient to recover the original matrix completely.
\end{theorem}
\begin{proof}
Later(According to my idea)
\end{proof}

\subsection{Inequalities in Matrix Completion}
In this section, we will explore and present various inequalities related to the number of missing elements in matrix completion, drawing from prominent papers.



{
%\singlespacing
\backmatter
\bibliographystyle{unsrt}
\bibliography{references}
}

\end{document}







