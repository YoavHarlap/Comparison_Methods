\documentclass[12pt, a4paper, twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\setcounter{secnumdepth}{3}
\numberwithin{equation}{chapter}
\numberwithin{figure}{chapter}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage[a4paper, inner=3cm]{geometry}
\usepackage{color, soul}
\usepackage[super]{nth}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{color}
\usepackage{subcaption}

\usepackage{mathtools}

\usepackage{booktabs}
\usepackage[svgnames,table]{xcolor}
\usepackage[tableposition=above]{caption}
\usepackage{pifont}

\newcommand{\rev}[1]{{{#1}}}
\newcommand{\re}[1]{{{#1}}}

\usepackage[hang, flushmargin]{footmisc}

\usepackage[section]{placeins}
\usepackage{float}
\usepackage{setspace}
\DeclareMathOperator{\Unif}{Unif}
\usepackage{tikz}


\usetikzlibrary{backgrounds}
\makeatletter

\tikzset{%
  fancy quotes/.style={
    text width=\fq@width pt,
    align=justify,
    inner sep=1em,
    anchor=north west,
    minimum width=\linewidth,
  },
  fancy quotes width/.initial={.8\linewidth},
  fancy quotes marks/.style={
    scale=8,
    text=white,
    inner sep=0pt,
  },
  fancy quotes opening/.style={
    fancy quotes marks,
  },
  fancy quotes closing/.style={
    fancy quotes marks,
  },
  fancy quotes background/.style={
    show background rectangle,
    inner frame xsep=0pt,
    background rectangle/.style={
      fill=gray!25,
      rounded corners,
    },
  }
}

\newenvironment{fancyquotes}[1][]{%
\noindent
\tikzpicture[fancy quotes background]
\node[fancy quotes opening,anchor=north west] (fq@ul) at (0,0) {``};
\tikz@scan@one@point\pgfutil@firstofone(fq@ul.east)
\pgfmathsetmacro{\fq@width}{\linewidth - 2*\pgf@x}
\node[fancy quotes,#1] (fq@txt) at (fq@ul.north west) \bgroup}
{\egroup;
\node[overlay,fancy quotes closing,anchor=east] at (fq@txt.south east) {''};
\endtikzpicture}

\makeatother

\newcommand*{\BeginNoToc}{%
  \addtocontents{toc}{%
    \edef\protect\SavedTocDepth{\protect\the\protect\value{tocdepth}}%
  }%
  \addtocontents{toc}{%
    \protect\setcounter{tocdepth}{-10}%
  }%
}
\newcommand*{\EndNoToc}{%
  \addtocontents{toc}{%
    \protect\setcounter{tocdepth}{\protect\SavedTocDepth}%
  }%
}

%\usepackage[Lenny]{fncychap}

\raggedbottom
%\DeclareMathOperator{\IDFT}{IDFT}

\usepackage[nottoc]{tocbibind}
\usepackage{fancyhdr}

\pagestyle{fancy}
\newcommand{\fncyfront}{%
\fancyhead[RO]{{\footnotesize \rightmark }}
\fancyfoot[RO]{\thepage }
\fancyhead[LE]{\footnotesize {\leftmark }}
\fancyfoot[LE]{\thepage }
\fancyhead[RE, LO]{}
\fancyfoot[C]{}
\renewcommand{\headrulewidth}{0.3 pt}}
\newcommand{\fncymain}{%
\fancyhead[RO]{{\footnotesize \rightmark}}
\fancyfoot[RO]{\thepage }
\fancyhead[LE]{{\footnotesize \leftmark}}
\fancyfoot[LE]{\thepage }
\fancyfoot[C]{}
\renewcommand{\headrulewidth}{0.3 pt}}

\pagestyle{empty}
\newenvironment{abstract}%
{\cleardoublepage \null \vfill \begin{center}%
\bfseries \abstractname \end{center}}%
{\vfill \null}

\usepackage{sectsty}
\allsectionsfont{\sffamily}

%\usepackage{appendix}

\usepackage[titletoc]{appendix}

\usepackage{natbib}
\usepackage{graphicx}

\usepackage{amsthm}
%\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\usepackage{algorithm}
\usepackage{algorithmicx}

%\usepackage{pdfpages}

\usepackage{hyperref}

\hypersetup{
	colorlinks,
	citecolor=blue,
	filecolor=black,
	linkcolor=blue,
	urlcolor=blue
}

%opening
\title{Comparative Analysis of Phase Retrieval and Matrix Completion}
\author{Yoav Harlap\\ Under the supervision of Prof. Tamir Bendory}

\begin{document}
\pagestyle{fancy}
\fncyfront
\frontmatter
\maketitle
\begin{abstract}
This thesis considers the phase retrieval and matrix completion problems, both aiming to reconstruct vectors and matrices from incomplete or corrupted observations, with a focus on leveraging iterative and projection-based algorithms. We are interested in scenarios where some additional information about the objects being reconstructed can be assumed. Inspired by the challenge of structural reconstruction, our goal is to determine the global phase vector from the Fourier magnitude data collected by the sensor. We will discuss matrix completion with a focus on low-rank recovery and explore interesting applications. We will present different algorithms that solve these difficult problems. We will evaluate the performance of each one, including its advantages and disadvantages in various cases. We will provide many numerical results and conclusions to offer insight and conceptual value to those who would like to solve similar challenges. The code is available at: ~\url{https://github.com/YoavHarlap/Comparison_Methods}.
\end{abstract}
\chapter*{Acknowledgments}
123456789.
{
\singlespacing
  \hypersetup{linkcolor=black}
  \tableofcontents
\BeginNoToc
\newpage
\listoffigures
%\newpage
%\listoftables
\EndNoToc
}
\onehalfspacing
\fncymain
\mainmatter
\chapter{Introduction}
\label{ch:intro}
\section{Overview of Phase Retrieval}

Phase retrieval is a fundamental problem in various fields such as optics ~\cite{fienup1982phase, shechtman2015phase}, signal processing ~\cite{candes2013phaselift, balan2006signal}, crystallography ~\cite{millane1990phase, hauptman1986minimal}, and quantum mechanics ~\cite{goy2018experimental, gross2010quantum}. It involves reconstructing a signal or image from the magnitude of its Fourier transform, where the phase information is missing or inaccessible. Since many physical measurement devices can only capture intensity, which is proportional to the magnitude squared of a signal and not the phase, phase retrieval becomes essential for accurate reconstruction of the original signal or image.

% To understand how significant the phase loss is, the simulation in \ref{fig:phase_transition}  demonstrates how changes in phase affect the image, leading to a gradual improvement in clarity as the phase shifts from random to correct phase values, while the magnitude remains the same. Each frame in the image grid represents a 20\% increase in phase alignment until it reaches the original image.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\textwidth]{figures1/Phase Transition from Random to Correct Phase.png}
%     \caption[Phase Transition from Random to Correct Phase]{Simulation showing phase impact on image clarity as phase shifts from random to correct phase values, with constant magnitude. Each frame represents a 20\% increase in phase alignment until it reaches the correct phase values in the original image.}
%     \label{fig:phase_transition}
% \end{figure}



\section{Mathematical Problem Formulation}
We consider a matrix \mbox{$A \in \mathbb{C}^{m\times n}$}, which will be referred to as the sensing matrix and a magnitude vector \mbox{$b \in \mathbb{R}^{m}$}, with the understanding that b elements are non-negative. Our task in the phase retrieval problem is to solve the following system of equations successfully:
\begin{equation}
\label{eq:model}
|Ax_0| = b
\end{equation}
\begin{itemize}
    \item The absolute value is applied to each element of the vector \( v \) individually, such that \[|v| = (|v_1|, |v_2|, \dots, |v_n|)\] where \( |v_i| \) represents the absolute value of each entry in \( v \).

    \item The matrix \( A \) can serve various purposes. Often, \( A \) is the Discrete Fourier Transform (DFT) matrix, where multiplying a vector \( x \) with \( A \) performs the Fourier transform. In this case, we are interested in the scenario where \( m = n \). We can also consider cases where \( A \) is a real random matrix or a complex random matrix, with \( m > n \) in the random scenarios.
\end{itemize}
In many problems in the world, as well as in our problem, we can assume certain things about our solution, which we refer to as additional information. This additional information consists of the constraints that our solution must meet.

In addition, we can also define our problem in the following way, and the reason for doing so will become clearer in the next chapter (see Chapter[?????????????] \ref{chap:projectionSetsMethod}). We can look for a point \( x \in \mathcal{A} \cap \mathcal{B} \), where \( \mathcal{B} \) is the set of all signals that satisfy Equation \ref{eq:model} and is defined as:
\begin{equation}
\mathcal{B} = \left\{ y \in \mathbb{C}^m : |y| = b \right\}
\label{eq:model2}
\end{equation}
The set \( \mathcal{A} \) typically represents additional constraints known about our solution, such as sparsity or finite support.



% \section{Historical Context and Applications}

% Phase retrieval has a significant history, especially in X-ray crystallography. As noted by Walther~\cite{walther1963synthesis}, in the past, scientists faced the challenge of determining the structure of crystalline materials using X-ray diffraction patterns, which provide only intensity information. This missing phase information, known as the "phase problem," was a major obstacle to accurately determining atomic structures. while the magnitude gives us the energy distribution, the phase reveals the actual structure. The phase retrieval problem extends beyond filling in missing data; it’s about unlocking the complete potential of our measurements to understand the true nature of the object being studied.

% Over the years, phase retrieval has been crucial in many fields. In optics, it’s used for designing systems and reconstructing images from diffraction patterns~\cite{born1999principles}. In astronomy, it helps improve image resolution by correcting for atmospheric distortions. With advances like X-ray free-electron lasers and electron microscopy, phase retrieval remains vital.

\section{Challenges and Limitations in Phase Retrieval Algorithms}
Over the past decade, considerable scholarly focus has been directed toward the computational and theoretical dimensions of the phase retrieval problem. To streamline the intricacies of mathematical analysis, a trend emerged wherein researchers adopted an idealized model—one abstracted from practical applications—where the entries of \(A\) are independently and identically distributed (i.i.d.) according to a normal distribution or other similar statistical frameworks. We shall henceforth designate the endeavor of reconstructing a signal under these conditions as the *random phase retrieval problem*. Among the most prevalent algorithms for addressing this problem are those that rely on the minimization of non-convex loss functions, such as non-convex least squares, through the application of first-order gradient methods (e.g., see  \cite{7029630, chen2016solvingrandomquadraticsystems, cai2015optimalratesconvergencenoisy, 8049465}). Importantly, this body of work has yielded robust theoretical assurances, demonstrating that the non-convex nature of the problem is typically well-behaved when the number of measurements \(m\) significantly exceeds the dimensionality \(n\) of the signal being recovered \cite{Sun_2017, Chen_2019}.

Regrettably, it has become evident that the random phase retrieval problem presents significantly less complexity compared to the authentic phase retrieval problem, particularly when \(A\) is a matrix of the Fourier-type. For most practical phase retrieval applications, algorithms developed for the random phase retrieval framework prove inadequate: the inherent non-convexity of the problem does not exhibit benign behavior, and gradient-based methods often converge to local minima, far from any global solution (see the detailed analysis in \cite{elser2018benchmarkproblemsphaseretrieval}). Consequently, despite extensive contributions to this literature, these methods have wielded only marginal impact on real-world applications. In practice, heuristic algorithms dominate, including the hybrid input-output (HIO) \cite{fienup1982phase}, difference map \cite{Elser:03}, relaxed averaged alternating reflections \cite{luke2004relaxed,Luke_2004}, and relaxed reflect reflect (RRR) \cite{elser2017matrix}. Each of these methods may be viewed as an extension of the Douglas-Rachford algorithm \cite{douglas1956numerical}; an introduction to these algorithms is provided in Section [later???]]. By employing a slight terminological generalization, we shall refer to these approaches as Douglas-Rachford type algorithms. While
these algorithms demonstrate strong empirical performance, their properties in the context of the non-convex phase retrieval problem remain largely unexplored.

Several key challenges continue to hinder theoretical advancements in this domain, including:
\begin{itemize}
    \item \textbf{Non-Uniqueness:} Different signals can share the same Fourier magnitude, leading to ambiguity ~\cite{Candes_2015}.
    \item \textbf{Noise Sensitivity:} Noise in measurements can significantly affect the reconstruction quality~\cite{elser2018benchmarkproblemsphaseretrieval}.
    \item \textbf{Computational Complexity:} Solving phase retrieval problems often requires complex, computationally demanding algorithms~\cite{Li_2017}.
     \item \textbf{Non-Convexity:} The solution space is non-convex, making optimization methods prone to local minima~\cite{Sun_2017,Chen_2019}. Both the alternating projection technique and gradient-based methods often struggle to yield meaningful solutions; they tend to converge rapidly to suboptimal local minima, rather than reaching a solution point. In practice, a set of algorithms that can be viewed as extensions of the Douglas-Rachford scheme is commonly used.
\end{itemize}



% \section{Intuitive Understanding and Challenges in Phase Retrieval}

% The phase retrieval problem can be understood intuitively as follows: Given the magnitude of the Fourier transform of a signal, how do we recover the signal? The challenge arises because the Fourier phase is lost during measurement. Thus, reconstructing the signal involves dealing with incomplete information.

% % Key challenges include:
% % \begin{itemize}
% %     \item \textbf{Non-Uniqueness:} Different signals can share the same Fourier magnitude, leading to ambiguity ~\cite{Candes_2015}.
% %     \item \textbf{Noise Sensitivity:} Noise in measurements can significantly affect the reconstruction quality~\cite{elser2018benchmarkproblemsphaseretrieval}.
% %     \item \textbf{Computational Complexity:} Solving phase retrieval problems often requires complex, computationally demanding algorithms~\cite{Li_2017}.
% %      \item \textbf{Non-Convexity:} The solution space is non-convex, making optimization methods prone to local minima~\cite{Sun_2017,Chen_2019}. Both the alternating projection technique and gradient-based methods often struggle to yield meaningful solutions; they tend to converge rapidly to suboptimal local minima, rather than reaching a solution point. In practice, a set of algorithms that can be viewed as extensions of the Douglas-Rachford scheme is commonly used.
% % \end{itemize}
\section{Matrix Completion - addddddddddddddddddddd connection}
Similar to phase retrieval, matrix completion also aims to reconstruct data from incomplete information but faces unique challenges. Phase retrieval recovers phase from magnitude data, while matrix completion fills in missing entries. Our algorithms are designed to address both problems, as you will see later.
One of the most common examples for matrix completion is the Sudoku puzzle shown in Figure \ref{fig:sudokuExample}.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures1/sudoku_1}
    \caption{Original Sudoku puzzle}
    \label{fig:sudokuOriginal}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures1/sudoku_2}
    \caption{Elements to be reconstructed (in blue)}
    \label{fig:sudokuMissing}
  \end{subfigure}
  \caption[An example of a matrix completion problem in Sudoku]{An example of a matrix completion problem is the Sudoku game. We are given the initial clues (the elements we know) and need to reconstruct the missing elements (the cells painted in blue).}
  \label{fig:sudokuExample}
\end{figure}


Matrix completion has broad applications across different fields:

\begin{itemize}
    \item \textbf{Recommender Systems:} A well-known application of matrix completion is in collaborative filtering for recommender systems, where users typically rate only a small fraction of available items. The missing ratings can be inferred based on the observed data and used to recommend items users are likely to enjoy (e.g., movies - see Netflix Matrix Model in~\ref{table:Netflix_table}, products) \cite{candes2008exactmatrixcompletionconvex}.

    \item \textbf{Image Processing:} In image inpainting, matrix completion helps to recover missing or corrupted pixels by treating the image as a matrix. Often, images exhibit a low-rank structure, which allows for effective restoration by completing the missing matrix entries based on this low-rank property \cite{JMLR:v11:mazumder10a}.

    \item \textbf{System Identification:} In fields like control systems, missing data due to sensor faults or communication errors can be problematic. Matrix completion can reconstruct incomplete measurement data, enabling more reliable system identification and control \cite{keshavan2009matrixcompletionentries}.
\end{itemize}


\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
 & \textbf{The Lion King} & \textbf{Avatar} & \textbf{Inception} & \textbf{Titanic} & \textbf{The Avengers} \\
\hline
\textbf{John} & 99\% & 85\% & 80\% & - & 75\% \\
\hline
\textbf{Emily} & - & 10\% & - & 5\% & 77\% \\
\hline
\textbf{Michael} & 99\% & 87\% & 12\% & 90\% & 80\% \\
\hline
\textbf{Sarah} & 91\% & - & 1\% & 97\% & - \\
\hline
\end{tabular}
\caption[Netflix Matrix Model]{Netflix Matrix Model, users select certain movies (elements) as priorities, while other elements in the matrix are completed based on our algorithms. Additional information, such as details about our data structure, clustering methods,matrix rank and other relevant components, is also considered in the model.}
\label{table:Netflix_table}
\end{table}


The foundational approach to matrix completion involves low-rank matrix recovery, based on the assumption that the complete matrix \( M \in \mathbb{R}^{m \times n} \) is of low rank \( r \), where \( r \ll \min(m, n) \). This low-rank condition implies that the matrix has inherent redundancy, allowing the observed data to be effectively represented with fewer components than the total number of entries.

Key Reasons for Using Low-Rank Matrix Completion:

\begin{itemize}
    \item \textbf{Data Reduction and Compact Representation:} Low-rank matrices allow for a compact representation of data, which reduces the number of parameters required to describe it. This property is crucial in matrix completion, as it implies that the observed entries contain enough information to reconstruct the entire matrix, even if many entries are missing.

    \item \textbf{Patterns and Redundancy:} Many types of real-world data exhibit low-rank structures. For example, in recommender systems, the preferences of users tend to cluster, so the user-item matrix is typically low-rank. Similarly, in images, spatial coherence causes neighboring pixels to be highly correlated, making images well-suited for low-rank assumptions.

    \item \textbf{Theoretical Guarantees for Recovery:} With sufficient samples and under specific conditions (such as incoherence and random sampling), low-rank matrix completion can recover the missing entries with high probability \cite{candes2008exactmatrixcompletionconvex}. This means that, theoretically, a low-rank matrix can be exactly reconstructed from a subset of its entries, which is not generally possible for higher-rank matrices.

    \item \textbf{Computational Efficiency:} Exploiting the low-rank structure leads to efficient algorithms for matrix completion. For instance, nuclear norm minimization—a convex relaxation of rank minimization—enables the use of efficient optimization techniques that make matrix completion feasible for large-scale data \cite{JMLR:v11:mazumder10a}.
\end{itemize}

\section{Research Scope and Objectives}
This thesis focuses on evaluating the performance of existing algorithms across various scenarios to compare their effectiveness in different contexts. By examining these scenarios, it aims to provide insights into the strengths and limitations of these algorithms under diverse conditions, bringing up the three main scenes:
\begin{itemize}
    \item \textbf{Phase retrieval with unknown phase information:} Comparing how algorithms perform when the phase is unknown and only the magnitude of the Fourier transform is provided.
    \item \textbf{Phase Retrieval with Random Matrices:} Assessing algorithm performance when the matrix $A$ is random rather than a Fourier transform. This encompasses practical scenarios involving real-valued scenes and complex-valued composite scenes.
    \item \textbf{Matrix Completion:} ============= \textbf{connect} to phase3333 33333 Investigating algorithms for matrix completion when some matrix elements are missing. This involves using knowledge of the matrix rank and initial clues (i.e., entries that are not missing) to recover the missing elements.
\end{itemize}

% \section{Structure of the Thesis}



% The thesis is structured as follows:
% \begin{itemize}
%     \item \textbf{Chapter 2:} Covers the mathematical foundations of phase retrieval and matrix completion, defining the problems and discussing their connection.

%     \item \textbf{Chapter 3:} Presents algorithms for phase retrieval and matrix completion, accompanied by visual examples to illustrate their application.
%     \item \textbf{Chapter 4:} ---
%     \item \textbf{Chapter 5:} ---
%     \item \textbf{Chapter 6:} Concludes the thesis with a summary of findings, contributions, and recommendations for future research.
% \end{itemize}

% \chapter{Mathematical Formulation and Explanations}
% \section{Problem Definition: Rigorous Approach}
% We consider a matrix \mbox{$A \in \mathbb{C}^{mxn}$}, which will be referred to as the sensing matrix and a magnitude vector \mbox{$b \in \mathbb{R}^{m}$}, with the understanding that b elements are non-negative. Our task in the phase retrieval problem is to solve the following system of equations successfully:
% \begin{equation}
% \label{eq:model}
% |Ax_0| = b
% \end{equation}
% \begin{itemize}
%     \item The absolute value is applied entry-wise: \( |v| \) where \( |v_{i}| \) represents the absolute value of each entry in the vector \( v \).

%     \item The matrix \( A \) can serve various purposes. Often, \( A \) is the Discrete Fourier Transform (DFT) matrix, where multiplying a vector \( x \) with \( A \) performs the Fourier transform. In this case, we are interested in the scenario where \( m = n \). We can also consider cases where \( A \) is a real random matrix or a complex random matrix, with \( m \gg n \) in the random scenarios.
% \end{itemize}
% In many problems in the world, as well as in our problem, we can assume certain things about our solution, which we refer to as additional information. This additional information consists of the constraints that our solution must meet.

% In addition, we can also define our problem in the following way, and the reason for doing so will become clearer in the next chapter (see Chapter \ref{chap:projectionSetsMethod}). We can look for a point \( x \in \mathcal{A} \cap \mathcal{B} \), where \( \mathcal{B} \) is the set of all signals that satisfy Equation \ref{eq:model} and is defined as:
% \begin{equation}
% \mathcal{B} = \left\{ y \in \mathbb{C}^m : |y| = b \right\}
% \label{eq:model2}
% \end{equation}
% The set \( \mathcal{A} \) typically represents additional constraints known about our solution, such as sparsity or finite support.

% \section{Matrix Completion}
% Matrix completion is a method used to reconstruct a matrix from a subset of its entries. It is widely applied in fields like signal processing, recommendation systems, and image processing.

% The foundational approach to matrix completion involves low-rank matrix recovery, initially addressed by Candes and Recht~\cite{candes2009exact} through nuclear norm minimization. This technique minimizes the nuclear norm (the sum of singular values) of the matrix, which serves as a convex approximation of the rank function.

% Recent advancements in matrix completion have introduced techniques that incorporate additional structural information or constraints. These methods extend classical algorithms to address new scenarios and requirements. Additionally, integrating machine learning and adaptive techniques has shown potential for enhancing performance and generalization. One of the most common examples for matrix completion is the Sudoku puzzle shown in Figure \ref{fig:sudokuExample}.


% \begin{figure}[ht]
%   \centering
%   \begin{subfigure}[b]{0.45\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figures1/sudoku_1}
%     \caption{Original Sudoku puzzle}
%     \label{fig:sudokuOriginal}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.45\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figures1/sudoku_2}
%     \caption{Elements to be reconstructed (in blue)}
%     \label{fig:sudokuMissing}
%   \end{subfigure}
%   \caption[An example of a matrix completion problem in Sudoku]{An example of a matrix completion problem is the Sudoku game. We are given the initial clues (the elements we know) and need to reconstruct the missing elements (the cells painted in blue).}
%   \label{fig:sudokuExample1}
% \end{figure}


% \section{Phase Retrieval versus Matrix Completion}
% Phase retrieval and matrix completion both reconstruct data from incomplete information but face unique challenges. Phase retrieval recovers phase from magnitude data, while matrix completion fills in missing entries. Comparative analysis reveals differences in noise handling, optimization, and complexity. Key metrics include reconstruction accuracy, efficiency, and noise robustness. This thesis will build on these insights to evaluate and compare algorithms for both problems, exploring various scenarios and discussing performance factors.Our algorithms are designed to address both problems, although we will customize the projections specifically for each case.Our algorithms are designed to address both problems, although we will customize the projections specifically for each case.


\chapter{Projection on Sets Method and Algorithms}
\label{chap:projectionSetsMethod}


\section{Detailed Projections for Case Studies}
In this section, we provide a comprehensive overview of the specific projection operations employed for each scenario analyzed in this paper, outlining how these projections are applied across various cases to solve the phase retrieval and matrix completion problems. As we have mentioned numerous times, the solution to our problem is to find the intersection point  \( x_0 \in \mathcal{A} \cap \mathcal{B} \). To address this, we define projectors onto these sets, with the requirement that for practical algorithms, these projectors must be efficiently computable.
The main cases we will discuss are:

\begin{enumerate}
    \item The matrix \( A \) in the model defined in \(\ref{eq:model}\) is a random matrix, where every element is drawn i.i.d. from a normal distribution, and it can be either real or complex. This case is called the "random case." It is interesting, even though it is considered easier, because many researchers start with this case before addressing the next, more complex scenarios.

    \item Phase retrieval definitions, as formulated in \cite{elser2018benchmarkproblemsphaseretrieval}, are practical, particularly in the challenge of structural reconstruction in single-particle Cryo-Electron Microscopy. In this case, our matrix \( A \) is the DFT matrix, and thus we refer to this case as the "DFT case." As explained in \cite{elser2018benchmarkproblemsphaseretrieval}, we can assume that our signal is sparse, which will be relevant for one of our projections later.

    \item Matrix completion: In this case, one projector ensures that the initial elements are imposed, and the second projector usually ensures that the rank of the matrix is \( r \). This case is referred to as "Matrix Completion."
\end{enumerate}




For a general \( x \in \mathbb{C}^n \), let \( y = Ax \in \mathbb{C}^m \). We focus on projectors in terms of \( y \) rather than \( x \) because computing the projector onto \( \mathcal{B} \) is significantly less expensive \cite{Li_2017, levin2020notedouglasrachfordgradientsphase}. In the random and the DFT cases, the projector of \( y \) onto the set \( \mathcal{B} \) is defined as

\[
P_{\mathcal{B}}(y) = b \odot \text{phase}(y),
\]

where \( b \) represents the measured magnitudes (see equation \ref{eq:model}), \( \odot \) denotes the point-wise product, and the phase operator is defined element-wise as

\[
\text{phase}(y)[i] := \frac{y[i]}{|y[i]|}, \quad y[i] \neq 0,
\]

and zero otherwise.


In the random case, the entries of the sensing matrix are typically drawn i.i.d. from a normal distribution with \( m > 2n \). A point \( y_0 \in \mathbb{C}^m \) that represents a solution should lie within the column space of the matrix \( A \), which means \( y_0 = A A^\dagger y_0 \), where \( A^\dagger \) denotes the pseudo-inverse of \( A \). The column space of \( A \), also known as the image of \( A \), includes all vectors that can be expressed as \( A x \) for some vector \( x \).

Thus, for any vector \( y \in \mathbb{C}^m \) to be a valid solution, it must be in the image of \( A \), meaning there exists a vector \( x \) such that \( y = A x \). Specifically, the projection of \( y \) onto the column space of \( A \) is given by:

\[
P_A(y) = A A^\dagger y.
\]

In the DFT case, we assume the additional information discussed earlier in the paper: the signal is known to be sparse. As a result, we impose a sparsity constraint on the solution (see \cite{elser2018benchmarkproblemsphaseretrieval}), in addition to the projection on B that we mentioned earlier.

Sparsity refers to the property where most elements of a vector are zero, with only a few non-zero entries. Specifically, for a vector \( x \in \mathbb{C}^n \), we say it is \( S \)-sparse if it has at most \( |S| \) non-zero entries. In this case, the second projection imposes the assumption of sparsity by retaining the \( |S| \) highest-magnitude elements of the vector and setting the rest to zero.

The projection operator \( P_S \) onto the space of \( S \)-sparse vectors is defined as follows: for a given vector \( x \in \mathbb{C}^n \), \( P_S(x) \) is obtained by:

\[
P_S(x)[i] =
\begin{cases}
x[i], & \text{if } i \text{ corresponds to one of the } |S| \text{ largest elements of } |x|, \\
0, & \text{otherwise}.
\end{cases}
\]

This projection ensures that the sparsity constraint is enforced on the signal, keeping the \( |S| \) largest-valued pixels unchanged while setting the rest to zero. By applying this projection iteratively, we can ensure that our solution aligns with the assumed sparsity of the signal.

\textbf{Note:} We impose the sparsity constraint on our vector \( x \) rather than on vector \( y \) as before. However, it is important to highlight that in the DFT case, the matrix \( A \) is invertible (IDFT). Therefore, it does not matter whether we apply the operations to \( y \) or \( x \), as long as we remain consistent in our approach.


As extensively discussed in the article by \cite{elser2018benchmarkproblemsphaseretrieval}, we will define the stopping conditions slightly differently here.
We set our stopping condition as follows:

\[
\frac{\text{I}_\text{S}}{\text{I}_\text{F}} > 0.95
\]

where \(\text{I}_\text{F}\), the power of the entire image (the sum of each pixel squared), is defined as:

\[
\text{I}_\text{F} = \sum_{x=0}^{M-1} \sum_{y=0}^{M-1} \rho^2(x, y),
\]

and \(\text{I}_\text{S}\), the power of the pixels in the support \( S \), is defined as:

\[
\text{I}_\text{S} = \sum_{(x,y) \in S} \rho^2(x, y).
\]

where \( \rho(x, y) \) represents the values in the reconstructed image, and \( S \) represents the support of the image.


We note that increasing the threshold value above 0.95 will lead to longer convergence times, but the performance of the reconstruction will improve in terms of accuracy.


\section{Projections for Matrix Completion Case}

In the case of Matrix Completion, the projections are different from the previous cases. The first projection enforces the known entries of the matrix, ensuring that the matrix matches the given values at specific locations. This can be represented using a mask operator that preserves the known entries and leaves the unknown entries unchanged. The second projection imposes a rank constraint, forcing the matrix to have a rank of \( r \), which is assumed based on additional information.

Let \( M \in \mathbb{R}^{m \times n} \) represent the matrix we want to complete, and let \( \Omega \) denote the set of indices where the matrix entries are known. We define the mask operator \( P_{\Omega} \) as follows:

\[
P_{\Omega}(X,M)[i,j] =
\begin{cases}
M[i,j], & (i,j) \in \Omega, \\
X[i,j], & (i,j) \notin \Omega,
\end{cases}
\]
where \( X \) is the matrix before projection. This projection ensures that the given matrix elements remain unchanged, while the unknown elements can still be updated in subsequent iterations.

The second projection forces the matrix to have a rank of \( r \). This is done using the Singular Value Decomposition (SVD). Suppose the matrix \( M \) has the SVD:

\[
M = U \Sigma V^*
\]
where \( \Sigma \) is the diagonal matrix of singular values, and \( U \), \( V \) are orthogonal matrices. The projection onto the set of rank-\( r \) matrices, denoted as \( P_r(M) \), is given by:

\[
P_r(M) = U \Sigma_r V^*
\]

where \( \Sigma_r \) is obtained by retaining only the largest \( r \) singular values and setting the rest to zero. This projection ensures that the resulting matrix has rank \( r \), while keeping the structure of the matrix close to the original.

This process is analogous to frequency filtering: by imposing the rank-\( r \) constraint, we eliminate the smallest singular values (which can be seen as high-frequency components), but the overall structure of the matrix remains largely unchanged.


\section{Defining and Analyzing Key Algorithms for Phase Retrieval}
In the following sections, we will explore several key algorithms that are crucial for solving the phase retrieval problem. We will begin by presenting their mathematical definitions and formulations. Specifically, we will examine the Hybrid Input-Output (HIO), Relaxed Averaged Alternating Reflections (RAAR), and  Relaxed Reflect Reflect  (RRR) algorithms. After defining these algorithms, we will discuss the specific projections used in each scenario under investigation. These iterative algorithms, based on projection techniques, serve as generalizations of the Douglas-Rachford algorithm. They play a significant role in tackling the challenges of phase retrieval by iteratively refining solutions through projection-based updates. This discussion will provide deeper insights into their practical applications and effectiveness.

Let \( y \in \mathbb{C}^n \) be a vector in \( n \)-dimensional complex space. We define the projection of \( y \) onto a set \( \mathcal{A} \) as \( P_\mathcal{A}(y) \), and the projection onto a set \( \mathcal{B} \) as \( P_\mathcal{B}(y) \). We will explicitly state later what each projection is, and it also depends on the specific problem we are solving. A solution is a point where the projections onto both sets $\mathcal{A}$ and $\mathcal{B}$ coincide, meaning that the projections of the point are in the intersection $\mathcal{A} \cap \mathcal{B}$.

\begin{definition}
A point \( y_0 \in \mathbb{C}^m \) is considered a solution if it satisfies the condition \( P_{\mathcal{A}}(y_0) = P_{\mathcal{B}}(y_0) \).
\end{definition}


In practice, many times we will be satisfied with equality up to a point of tolerance, or it will be used as a requirement in the noisy case. This condition will constitute the stopping criterion for our algorithms (except where otherwise stated). Intuitively, when we refer to projections as constraints, it is clear that if the projection of the vector onto one space is the same as the projection onto another space, then this is a solution. We have found a vector that satisfies all our constraints.





\section{Set Projection Example Illustrated with Sudoku}
Before presenting a rigorous discussion on the projection methods, an illustrative example using Sudoku will be provided. One of the methods to solve Sudoku is with the help of the projection method and by viewing the problem visually. Let's say that our large space is the space of all solutions for arranging numbers 1-9 in the cells of the Sudoku matrix. To solve the game, we have to obey several rules. The first rule is that we must use the initial clues we received (i.e., the numbers we already have at the beginning of the game). The second rule requires us to have the numbers 1-9 in each column, as well as in each row and block. We will say that each rule is a constraint on our solution. Each constraint defines a subspace within our large space. To solve the Sudoku, we will want to find the intersection of all these subspaces, which means that all our constraints are satisfied. Now, it is useful and understandable to define our problem as we have defined it in~\eqref{eq:model2}.

\begin{figure}[ht]
  \centering
  \resizebox{0.8\textwidth}{!}{ % Scale down the entire figure
    \begin{minipage}{\textwidth}
      \centering
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_1}
        \caption{Row constraint: Each row must contain the numbers 1-9.}
        \label{fig:sudokuRow}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_2}
        \caption{Column constraint: Each column must contain the numbers 1-9.}
        \label{fig:sudokuColumn}
      \end{subfigure}

      \vspace{0.5cm}

      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_3}
        \caption{Block constraint: Each of the 9 distinct 3x3 block must contain the numbers 1-9.}
        \label{fig:sudokuBlock}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_4}
        \caption{Solution: The completed Sudoku grid where all constraints are satisfied.}
        \label{fig:sudokuSolution}
      \end{subfigure}
    \end{minipage}
  }
  \caption[Examples of Sudoku constraints]{Examples of Sudoku constraints: row, column, block, and the solution. Each constraint requires that the numbers 1-9 be used exactly once in the respective row, column, or block.}
  \label{fig:sudokuConstraints}
\end{figure}
After understanding how to define a problem using spaces and subspaces, we want to find the intersection point using a computationally efficient method. More mathematically, we start with a random initial vector in our large space. We then define a subspace according to the constraint and project the vector onto this subspace. This process ensures that our vector satisfies the specific constraint. Our problem arises when we have multiple constraints (see \ref{fig:sudokuConstraints}): if we force our solution to fulfill one constraint and then impose it on the second subspace, it will, of course, satisfy the second constraint but not necessarily fulfill the first.
The naive method in our case to solve the Sudoku is to iteratively project each constraint one by one and hope it converges. That is, to project one constraint, then the second constraint, and so on, and then again the first constraint, and so on again (because the projection of the previous constraint did not preserve the first constraint). Later, we will see smarter ways to solve this using others methods of projections.


\chapter{Algorithm Iterative Steps}

In this chapter, we delve into the iterative mechanisms that underpin a selection of widely used algorithms. Starting from a random initial point in the solution space, each algorithm iteratively updates its estimate, leveraging mathematical operations designed to refine the solution progressively. The algorithms we will discuss include:

\begin{itemize}
    \item \textbf{Alternating Projections}
    \item \textbf{Hybrid Input-Output (HIO)}
    \item \textbf{Relaxed Averaged Alternating Reflections (RAAR)}
    \item \textbf{Relaxed Reflect Reflect (RRR)}
\end{itemize}

While the specifics of each algorithm will be detailed later, the unifying principles of \textit{projection}, \textit{reflection}, and \textit{relaxation} warrant a closer examination, as they form the foundation of the iterative steps employed by all the methods discussed.

\section*{Projection}
As explained in the previous chapter, projection refers to the operation of mapping a point onto a set, typically in such a way that the distance from the point to the set is minimized. In iterative algorithms, projection helps ensure that the solution remains consistent with a given constraint or set of constraints.

\section*{Reflection}
Reflection builds on the concept of projection by extending the operation. For a point \( x \) and a set \( S \), the reflection across \( S \) is given by
\[
R_S(x) = 2P_S(x) - x,
\]
where \( P_S(x) \) denotes the projection of \( x \) onto the set \( S \). Reflection is particularly useful in optimization and feasibility problems, as it can drive the solution away from invalid regions while amplifying the effect of constraints.

\section*{Relaxation}
Relaxation introduces a degree of flexibility to projection or reflection operations, allowing the algorithm to adjust the strength of these operations. For instance, in a relaxed projection, the operation may take the form
\[
x_{k+1} = x_k + \beta \big(P_S(x_k) - x_k\big),
\]
where \( \beta \) is a relaxation parameter that controls the step size or influence of the projection. Similarly, relaxation can be applied to reflections, offering a balance between aggressive and conservative updates to the solution. This flexibility is critical for managing convergence behavior, especially in challenging optimization landscapes.

By combining these fundamental operations with iterative strategies, the algorithms presented in this chapter achieve robust and efficient convergence. The mathematical formulation of these methods will clarify their distinctions and similarities, paving the way for a detailed comparison of their performances and applications.

\subsection{Alternating Projections}

The Alternating Projections method involves iteratively projecting onto different sets. The update step for this method is given by:
\begin{equation}
y^{(k+1)} = P_A \left( P_B \left( y^{(k)} \right) \right)
\end{equation}
where \( y^{(k)} \) is the vector at the \( k \)-th iteration, and \( P_A \) and \( P_B \) are the projection operators onto sets \( A \) and \( B \), respectively. This method alternates between projecting onto each set to iteratively approach a solution that satisfies all constraints. The main problem with this algorithm is that it often gets stuck at a particular point, as illustrated in Figure \ref{fig:algorithm_behaviors}.



\begin{figure}[h!]
    \centering

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_1}
        \caption{Start from a random point}
        \label{fig:proj_circle_1_1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_2}
        \caption{Projection on the first circle}
        \label{fig:proj_circle_1_2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_3}
        \caption{Projection on the second circle}
        \label{fig:proj_circle_1_3}
    \end{minipage}

    \vspace{0.5cm}

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_4}
        \caption{Projection on the third circle}
        \label{fig:proj_circle_1_4}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_5}
        \caption{First step of the algorithm}
        \label{fig:proj_circle_1_5}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_6}
        \caption{Convergence to the intersection point}
        \label{fig:proj_circle_1_6}
    \end{minipage}

    \caption[Illustration of an Alternating Projections algorithm using orthogonal projections.]{Illustration of an Alternating Projections algorithm. The figure shows iterations in a projection-based algorithm within a 2-dimensional space. The goal is to find the intersection point of three circles using orthogonal projections. The process starts from a random point and proceeds iteratively. These images are referenced from the lecture by Andrew Maiden in 2022 \cite{Maiden2022}.}
    \label{fig:proj_circle}
\end{figure}

\noindent In Figure \ref{fig:proj_circle}, an illustration of iterations in a projection-based algorithm in a 2-dimensional space is presented. In this example, the goal was to find the intersection of three circles using orthogonal projections onto them. The process began from a random point and continued iteratively until convergence to the solution. It is important to note that there are cases where convergence to a solution may not occur quickly or, in some cases, may not occur??????????????????? at all. This images is referenced from the lecture by Andrew Maiden in 2022 \cite{Maiden2022}.



\subsection{Hybrid Input-Output (HIO)}

The Hybrid Input-Output (HIO) algorithm updates its iterative step as follows:
\begin{equation}
y^{(k+1)} = y^{(k)} + P_A \left( (1 + \alpha)P_B(y^{(k)}) - y^{(k)} \right) - \alpha P_B \left( y^{(k)} \right)
\end{equation}
where \( \alpha \) is a parameter controlling the amount of feedback applied in the iteration process.

\subsection{Relaxed Averaged Alternating Reflections (RAAR)}

The Relaxed Averaged Alternating Reflections (RAAR) algorithm updates its iterative step as:
\begin{equation}
y^{(k+1)} = \beta   \left(y^{(k)}+ P_A \left( 2P_B(y^{(k)})- y^{(k)}\right) \right) +
(1-2\beta)  P_B \left( y^{(k)} \right)
\end{equation}

where \( \beta \) is a parameter controlling the balance between the two projections \( P_{\mathcal{A}} \) and \( P_{\mathcal{B}} \).

\subsection{Relaxed Reflect Reflect (RRR)}

The  Relaxed Reflect Reflect (RRR) algorithm updates its iterative step as:
\begin{equation}
y^{(k+1)} = y^{(k)} +\gamma  \left( P_A \left( 2P_B(y^{(k)})- y^{(k)}\right) -  P_B \left( y^{(k)} \right) \right)
\end{equation}

where \( \gamma \) is a parameter similar to \( \beta \) in RAAR, determining the weight given to each projection. One advantage of this algorithm is that it oscillates, which helps it avoid getting stuck in a local minimum, unlike the Alternating Projections (AP) method, see figure \ref{fig:algorithm_behaviors}. This oscillatory behavior often leads to excellent performance, as will be further discussed in the next chapter.
We will consider cases where \( \alpha \), \( \beta \), and \( \gamma \) lie in the interval \((0, 1)\).


\begin{figure}[h!]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/AP_example_stuck_1}
        \caption{Alternating Projections method getting stuck.}
        \label{fig:ap_stuck}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/RRR_example_oscillate_1}
        \caption{RRR algorithm exhibiting oscillations.}
        \label{fig:rrr_oscillate}
    \end{minipage}
    \caption[RRR and Alternating Projections algorithms exhibit different behaviors.]{Illustrations of different algorithm behaviors: (left) Alternating Projections method getting stuck and (right) RRR algorithm oscillating. This is an example of running a composite random matrix \(A\) drawn i.i.d. from a normal distribution with size \(50 \times 15\). The run was stopped after 1000 iterations when the Alternating Projections (AP) algorithm still did not converge at this stage, whereas the Relaxed Reflect Reflect (RRR) algorithm had already converged.
}
    \label{fig:algorithm_behaviors}
\end{figure}

\chapter{Theoretical Boundaries in Matrix Completion}
laterrrrrrrrrrrr
\section{Number of Missing Entries in Matrix Completion}
\subsection{The Netflix Matrix Model: Recommendations and Ratings Analysis}
The Netflix matrix model consists of rows representing users and columns representing movies, as detailed in Table ~\ref{table:Netflix_table}. Our objective is to recommend movies to users based on their preferences. This model incorporates data on how users rated movies, typically expressed as percentage values. Understanding how many entries need to be provided and how many can be missing is crucial for effective matrix completion. Additionally, the matrix rank r is considered, which provides insights into the underlying structure of the ratings. While examining whether this matrix rank reflects mathematical patterns—such as the tendency of similar users to enjoy similar movies or the preference of individuals for specific genres—is intriguing, it is beyond the scope of our current discussion.

% \begin{table}[h!]
% \centering
% \begin{tabular}{|l|c|c|c|c|c|}
% \hline
%  & \textbf{The Lion King} & \textbf{Avatar} & \textbf{Inception} & \textbf{Titanic} & \textbf{The Avengers} \\
% \hline
% \textbf{John} & 99\% & 85\% & 80\% & - & 75\% \\
% \hline
% \textbf{Emily} & - & 10\% & - & 5\% & 77\% \\
% \hline
% \textbf{Michael} & 99\% & 87\% & 12\% & 90\% & 80\% \\
% \hline
% \textbf{Sarah} & 91\% & - & 1\% & 97\% & - \\
% \hline
% \end{tabular}
% \caption[Netflix Matrix Model]{Netflix Matrix Model, There are elements in the matrix that need to be completed based on our algorithms}
% \label{table:Netflix_table1}
% \end{table}

\subsection{Maximum Allowable Number of Missing Elements in Matrix Completion}

In this section, we investigate the recovery of a matrix from incomplete data. Specifically, we consider the scenario where we can delete up to \(({n-r})^2\) elements and still recover the matrix in full, where \(r\) denotes the rank of the matrix. This scenario is optimal and significant because removing one more element beyond this point would make reconstruction impossible.

\begin{theorem}
Let \( \mathbf{M} \) be a matrix of rank \( r \) with dimensions \( n \times n \). If we delete up to \(({n-r})^2\) elements from \( \mathbf{M} \), the remaining elements are sufficient to recover the original matrix completely.
\end{theorem}
\begin{proof}
Later(According to my idea)
\end{proof}

\subsection{Inequalities in Matrix Completion}
In this section, we will explore and present various inequalities related to the number of missing elements in matrix completion, drawing from prominent papers.

\chapter{Numerical experiments}
In this chapter, I will present the results of my research, using graphs and simulations to clarify and illustrate the findings. I will refer to different cases to provide context and depth to the analysis, comparing various algorithms to highlight their performance in each scenario. Additionally, I will explain and interpret the data, offering insights and a detailed discussion supported by the simulations I built in the attached code.
\section{Matrix Completion}
I will begin by illustrating how the basic simulation was performed, using a simple example to lay the groundwork for understanding the methodology. Once the foundation is established, we will delve into more advanced concepts, exploring their intricacies and implications in greater detail.
As we know, our problem involves reconstructing a correct matrix, which I will refer to as the True matrix, using the non-missing elements provided, along with the additional information about the matrix's rank. This framework serves as the basis for our approach, guiding the reconstruction process to ensure accuracy and alignment with the given constraints.


In the example shown in Figures \ref{fig:example_matrices} and 3, you can see exactly how the simulation is performed. First, we created an $11 \times 11$ matrix with rank 3 and randomly selected 25 indices from the matrix to discard. We then provided two inputs to our code. The first input is the matrix we created after removing the 25 elements corresponding to the selected indices (this matrix is called the \textit{hint matrix}), and the second input is the information about the rank of the matrix, which in our case is 3.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/true.png}
        \caption{The true matrix (original matrix).}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/given.png}
        \caption{The hint matrix, along with the additional information that the matrix has a rank of 3, was provided.}
    \end{subfigure}
    \caption{Illustration of the original and hint matrices used in the simulation.}
    \label{fig:example_matrices}
\end{figure}
After receiving the inputs, we will begin solving our problem using the RRR algorithm to reconstruct the complete matrix. The process starts from a random initialization, as shown in Figure~\ref{fig:init_matrix}. For convenience, an element in the matrix will be colored green if it is similar to the value in the original matrix and red if it is different.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Matrix_Completion/Example_of_running_2_matrices/init.png}
    \caption{The initial random matrix used as the starting point for the RRR algorithm.}
    \label{fig:init_matrix}
\end{figure}

In the following figure, we present the first iteration, the 33rd iteration, and the final iteration of the RRR algorithm. This visualization demonstrates the convergence process, where more elements of the matrix progressively approach the values of the original matrix, turning green. By the 33rd iteration, a significant number of elements are already close to the original values, highlighting the algorithm's progress. It is important to note that the graphs reflect the state of the matrix after enforcing the hint matrix projection. As a result, the red elements appear only in the missing entries, where there is no information from the hint matrix.

\begin{figure}[p]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/1th_iter.png}
        \caption{Iteration 1.}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/33th_iter.png}
        \caption{Iteration 33.}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/end.png}
        \caption{Final iteration.}
    \end{subfigure}
    \caption[Visualization of the first, 33rd, and final iterations of the matrix in the RRR algorithm]{Visualization of the first, 33rd, and final iterations of the matrix in the RRR algorithm, showing the progression of convergence. Green elements indicate agreement with the original matrix, while red elements appear only in the missing entries after enforcing the hint matrix projection.}
    \label{fig:iterations}
\end{figure}


In addition to the visualization of matrix convergence, we evaluate the performance of the RRR algorithm graphically. Two metrics are plotted to provide insights into the algorithm's behavior across iterations. The first graph in Figure~\ref{fig:performance_plots} depicts the convergence of the algorithm by plotting the norm difference, \( |PB(y, b) - PA(y, A)| \), as a function of the iteration number. This graph is particularly significant, as convergence or a solution is defined when the projections are identical, or in our case, sufficiently similar within a tolerance of \(10^{-6}\).
The second graph in the same figure illustrates the norm difference between the reconstructed matrix and the original true matrix, \( |true\_matrix - iter\_matrix| \), highlighting the accuracy of the reconstruction over iterations. Together, these plots provide a comprehensive view of the convergence and effectiveness of the algorithm.

This example represents a relatively simple case, designed to demonstrate how the simulation operates. In the following sections, we will focus on more complex scenarios, where we are particularly interested in analyzing convergence or divergence, along with statistics and percentages derived from a larger set of experiments. These experiments will explore a broader range of parameter variations, such as matrix size, rank, and the proportion of missing entries.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=0.80\textwidth]{Matrix_Completion/Example_of_running_2_matrices/convergence.png}
        \caption{Projection error: \\ $|PB(y, b) - PA(y, A)|$ over iterations.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=0.80\textwidth]{Matrix_Completion/Example_of_running_2_matrices/convergence2.png}
        \caption{Reconstruction error: \\ $|true\_matrix - iter\_matrix|$ over iterations.}
    \end{subfigure}
    \caption[Convergence and reconstruction errors of the RRR algorithm]{Convergence and reconstruction errors of the RRR algorithm. The first graph shows the error between projections at each iteration, while the second shows how closely the reconstructed matrix approximates the original true matrix.}
    \label{fig:performance_plots}
\end{figure}



\subsection{Comparison of Matrix Completion Algorithms}

Now we investigate the performance of three matrix completion algorithms: \textbf{Alternating Projections}, \textbf{RRR}, and \textbf{RAAR}. The goal is to evaluate their ability to converge under a specific experimental setup. In this scenario, the matrix has a size of $n = 20$, a rank of $r = 3$, and $q = 50$ randomly missing elements.

To ensure a statistically robust analysis, the experiment was repeated $10,000$ times independently. The success rate of each algorithm—defined as the percentage of trials where convergence occurred—is summarized in Figure~\ref{fig:convergence}. These results provide valuable insights into the relative strengths of the algorithms under the given constraints.

From the results, it is evident that all three algorithms demonstrate high levels of reliability, with success rates consistently exceeding $97\%$. However, subtle differences between the algorithms offer meaningful insights. The \textbf{RRR} algorithm emerges as the most robust, achieving the highest convergence rate across all trials. This suggests that \textbf{RRR} is particularly well-suited for matrix completion tasks where a high level of precision and consistency is required.

The \textbf{RAAR} algorithm, while slightly less consistent than \textbf{RRR}, still performs remarkably well, demonstrating that it is a strong alternative in scenarios where computational constraints or other practical considerations might favor its use. Finally, \textbf{Alternating Projections}, despite its slightly lower success rate, provides a simpler approach that remains effective for most practical cases. This highlights a potential trade-off between algorithmic complexity and reliability.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Matrix_Completion/10000_trials/1}
    \caption[Convergence rates of the three algorithms over 10,000 trials.]{Convergence rates of the three algorithms over 10,000 trials. The graph shows the percentage of successful trials for each algorithm: Alternating Projections, RRR, and RAAR.}
    \label{fig:success_percentage}
\end{figure}

\subsection{Convergence Steps Analysis}

In addition to evaluating the overall success percentage, we also analyzed the number of iterations required for convergence in each trial. Figure~\ref{fig:iteration_analysis} provides a detailed view of this analysis. The graph on the left shows an overview of the iteration counts for all trials where convergence occurred, while the graph on the right zooms in to highlight specific trials and trends.

From these graphs, an important observation emerges: while the \textbf{RRR} algorithm achieved the highest overall convergence percentage, it does not necessarily converge the fastest. In cases where other algorithms also converged, they often did so in fewer iterations compared to \textbf{RRR}. This trade-off highlights a subtle difference in the behavior of these algorithms. For example, in the zoomed-in graph, it is evident that for a specific trial (e.g., Trial 8), no algorithm other than \textbf{RRR} managed to converge, further underscoring its robustness. However, the iteration counts in trials where convergence was achieved by multiple algorithms reveal that \textbf{Alternating Projections} and \textbf{RAAR} tend to require fewer steps, on average, to reach a solution.

It is important to note that cases where an algorithm did not converge are not represented in these graphs. Therefore, the percentage-based success results from Figure~\ref{fig:success_percentage} remain crucial for interpreting the overall effectiveness of the algorithms.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{Matrix_Completion/10000_trials/iteration_counts}
    \includegraphics[width=0.45\textwidth]{Matrix_Completion/10000_trials/iteration_zoom}
    \caption{(Left) Overview of iteration counts for all trials where convergence occurred. (Right) Zoomed-in view highlighting specific trials, such as Trial 8, where only \textbf{RRR} converged.}
    \label{fig:iteration_analysis}
\end{figure}

These findings emphasize the nuanced trade-offs between reliability and speed of convergence. While \textbf{RRR} remains the most robust algorithm, situations where fewer iterations are preferred might benefit from using \textbf{Alternating Projections} or \textbf{RAAR}, provided they meet the convergence requirements.

Overall, these findings illustrate that while all three algorithms are capable of addressing matrix completion problems, the choice of algorithm can be informed by the specific requirements of the task, such as computational efficiency or the need for maximal reliability. Future work could explore larger matrices, different rank configurations, or various levels of missing data to deepen our understanding of these algorithms' performance and scalability.











\chapter{Conclusion}


{
%\singlespacing
\backmatter
\bibliographystyle{unsrt}
\bibliography{references}
}

\end{document}







