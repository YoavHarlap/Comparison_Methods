\documentclass[12pt, a4paper, twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\setcounter{secnumdepth}{3}
\numberwithin{equation}{chapter}
\numberwithin{figure}{chapter}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage[a4paper, inner=3cm]{geometry}
\usepackage{color, soul}
\usepackage[super]{nth}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{color}
\usepackage{subcaption}

\usepackage{mathtools}

\usepackage{booktabs}
\usepackage[svgnames,table]{xcolor}
\usepackage[tableposition=above]{caption}
\usepackage{pifont}

\newcommand{\rev}[1]{{{#1}}}
\newcommand{\re}[1]{{{#1}}}

\usepackage[hang, flushmargin]{footmisc}

\usepackage[section]{placeins}
\usepackage{float}
\usepackage{setspace}
\DeclareMathOperator{\Unif}{Unif}
\usepackage{tikz}


\usetikzlibrary{backgrounds}
\makeatletter

\tikzset{%
  fancy quotes/.style={
    text width=\fq@width pt,
    align=justify,
    inner sep=1em,
    anchor=north west,
    minimum width=\linewidth,
  },
  fancy quotes width/.initial={.8\linewidth},
  fancy quotes marks/.style={
    scale=8,
    text=white,
    inner sep=0pt,
  },
  fancy quotes opening/.style={
    fancy quotes marks,
  },
  fancy quotes closing/.style={
    fancy quotes marks,
  },
  fancy quotes background/.style={
    show background rectangle,
    inner frame xsep=0pt,
    background rectangle/.style={
      fill=gray!25,
      rounded corners,
    },
  }
}

\newenvironment{fancyquotes}[1][]{%
\noindent
\tikzpicture[fancy quotes background]
\node[fancy quotes opening,anchor=north west] (fq@ul) at (0,0) {``};
\tikz@scan@one@point\pgfutil@firstofone(fq@ul.east)
\pgfmathsetmacro{\fq@width}{\linewidth - 2*\pgf@x}
\node[fancy quotes,#1] (fq@txt) at (fq@ul.north west) \bgroup}
{\egroup;
\node[overlay,fancy quotes closing,anchor=east] at (fq@txt.south east) {''};
\endtikzpicture}

\makeatother

\newcommand*{\BeginNoToc}{%
  \addtocontents{toc}{%
    \edef\protect\SavedTocDepth{\protect\the\protect\value{tocdepth}}%
  }%
  \addtocontents{toc}{%
    \protect\setcounter{tocdepth}{-10}%
  }%
}
\newcommand*{\EndNoToc}{%
  \addtocontents{toc}{%
    \protect\setcounter{tocdepth}{\protect\SavedTocDepth}%
  }%
}

%\usepackage[Lenny]{fncychap}

\raggedbottom
%\DeclareMathOperator{\IDFT}{IDFT}

\usepackage[nottoc]{tocbibind}
\usepackage{fancyhdr}

\pagestyle{fancy}
\newcommand{\fncyfront}{%
\fancyhead[RO]{{\footnotesize \rightmark }}
\fancyfoot[RO]{\thepage }
\fancyhead[LE]{\footnotesize {\leftmark }}
\fancyfoot[LE]{\thepage }
\fancyhead[RE, LO]{}
\fancyfoot[C]{}
\renewcommand{\headrulewidth}{0.3 pt}}
\newcommand{\fncymain}{%
\fancyhead[RO]{{\footnotesize \rightmark}}
\fancyfoot[RO]{\thepage }
\fancyhead[LE]{{\footnotesize \leftmark}}
\fancyfoot[LE]{\thepage }
\fancyfoot[C]{}
\renewcommand{\headrulewidth}{0.3 pt}}

\pagestyle{empty}
\newenvironment{abstract}%
{\cleardoublepage \null \vfill \begin{center}%
\bfseries \abstractname \end{center}}%
{\vfill \null}

\usepackage{sectsty}
\allsectionsfont{\sffamily}

%\usepackage{appendix}

\usepackage[titletoc]{appendix}

\usepackage{natbib}
\usepackage{graphicx}

\usepackage{amsthm}
%\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\usepackage{algorithm}
\usepackage{algorithmicx}

%\usepackage{pdfpages}

\usepackage{hyperref}

\hypersetup{
	colorlinks,
	citecolor=blue,
	filecolor=black,
	linkcolor=blue,
	urlcolor=blue
}

%opening
\title{Comparative Analysis of Phase Retrieval and Matrix Completion}
\author{Yoav Harlap\\ Under the supervision of Prof. Tamir Bendory}

\begin{document}
\pagestyle{fancy}
\fncyfront
\frontmatter
\maketitle
\begin{abstract}
This thesis considers the phase retrieval and matrix completion problems, both aiming to reconstruct vectors and matrices from incomplete or corrupted observations, with a focus on leveraging iterative and projection-based algorithms. We are interested in scenarios where some additional information about the objects being reconstructed can be assumed. Inspired by the challenge of structural reconstruction, we aim to derive the global phase vector from Fourier magnitude data, but will also tackle simpler problems. We will discuss matrix completion with a focus on low-rank recovery and explore interesting applications. We will present different algorithms that solve these difficult problems. We will evaluate the performance of each one, including its advantages and disadvantages in various cases. We will provide many numerical results and conclusions to offer insight and conceptual value to those who would like to solve similar challenges. The code is available at: ~\url{https://github.com/YoavHarlap/Comparison_Methods}.
\end{abstract}
\chapter*{Acknowledgments}
123456789.
{
\singlespacing
  \hypersetup{linkcolor=black}
  \tableofcontents
\BeginNoToc
\newpage
\listoffigures
%\newpage
%\listoftables
\EndNoToc
}
\onehalfspacing
\fncymain
\mainmatter
\chapter{Introduction}
\label{ch:intro}
\section{Overview and Motivation for Phase Retrieval}
Phase retrieval is a critical problem in several fields, including optics ~\cite{fienup1982phase, shechtman2015phase}, signal processing ~\cite{candes2013phaselift, balan2006signal}, crystallography ~\cite{millane1990phase, hauptman1986minimal}, and quantum mechanics ~\cite{goy2018experimental, gross2010quantum}. This process involves reconstructing a signal or image from the magnitude of its Fourier transform when the corresponding phase information is either missing or inaccessible. The issue arises because detectors are inherently limited to measuring the \textbf{intensity} (the magnitude squared) of a wave, while the \textbf{phase information} is not captured. However, the phase is essential for accurately reconstructing the spatial structure of an object. Without the phase, even with flawless intensity data, the reconstructed image becomes incomplete or distorted.

To understand the importance of phase in the context of image reconstruction, we first consider how an image can be represented in the Fourier domain. Any spatial image can be expressed as a combination of its Fourier magnitude (\(|F(u, v)|\)) and Fourier phase (\(\phi(u, v)\)):
\[
F(u, v) = |F(u, v)| e^{i\phi(u, v)}.
\]
Here, \(|F(u, v)|\) describes the energy distribution among the frequency components, while \(\phi(u, v)\) determines the spatial arrangement of those components.

Figure~\ref{fig:randomphase} demonstrates the significance of phase. The left image is the original "Astronaut" image, while the middle panel shows its Fourier magnitude. The right image reconstructs the original image using the correct magnitude but replacing the phase with random values. The resulting image becomes visually unrecognizable, appearing as noise. This highlights that the magnitude alone is insufficient for reconstruction without the phase, as it fails to preserve the spatial structure.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{intro/Figure_1.png}
    \caption[The role of phase in image reconstruction.]{The role of phase in image reconstruction. Left: Original image. Middle: Fourier magnitude. Right: Reconstruction with random phase. The images used are sourced from the open-access `scikit-image` Python library.}
    \label{fig:randomphase}
\end{figure}

To further illustrate the importance of phase, consider the reconstruction shown in Figure~\ref{fig:phasereplacement}. The leftmost image is the original astronaut image, while the middle image is reconstructed by combining the Fourier magnitude of the astronaut with the phase of a second image (coins, shown on the right). The reconstructed image clearly adopts the spatial structure of the coins rather than the astronaut, demonstrating that the phase predominantly dictates the spatial features of the reconstruction.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{intro/Figure_2.png}
    \caption[Phase replacement experiment.]{Phase replacement experiment. Left: Original astronaut image. Middle: Reconstruction using astronaut magnitude and coins phase. Right: Coins image (phase source). The images used are sourced from the open-access `scikit-image` Python library}
    \label{fig:phasereplacement}
\end{figure}

While phase is crucial in many cases, there are specific scenarios where the magnitude plays a more dominant role. Figure~\ref{fig:gridexample} illustrates this using a grid image. The left panel shows the original grid, which is a periodic, structured pattern. The middle panel is reconstructed using the grid’s magnitude combined with the phase of the coins image. Despite the phase change, the reconstruction retains the grid-like structure, indicating that the magnitude primarily defines the visual characteristics in such periodic cases.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{intro/Figure_3.png}
    \caption[Grid example: Magnitude dominance]{Grid example: Magnitude dominance. Left: Original grid image. Middle: Reconstruction using grid magnitude and a different phase. Right: Phase source image. The images used are sourced from the open-access `scikit-image` Python library}
    \label{fig:gridexample}
\end{figure}


These experiments provide intuition into the role of phase in image reconstruction:
In complex, natural images, the phase is critical for preserving the spatial structure.
However, in periodic and structured images like the grid, the magnitude can dominate, and the phase has a reduced impact.

Understanding the phase-magnitude interplay is essential for addressing phase retrieval problems and interpreting the limitations of intensity-only measurements.



\section{Mathematical Problem Formulation}
In the extended formulation of the problem, we consider a matrix \mbox{$A \in \mathbb{C}^{m\times n}$}, which will be referred to as the sensing matrix and a magnitude vector \mbox{$b \in \mathbb{R}^{m}$}, with the understanding that b elements are non-negative. Our task in the phase retrieval problem is to solve the following system of equations successfully:
\begin{equation}
\label{eq:model}
|Ax_0| = b
\end{equation}
\begin{itemize}
    \item The absolute value is applied to each element of the vector \( v \) individually, such that \[|v| = (|v_1|, |v_2|, \dots, |v_n|)\] where \( |v_i| \) represents the absolute value of each entry in \( v \).

    \item The matrix \( A \) can serve various purposes. Often, \( A \) is the Discrete Fourier Transform (DFT) matrix, where multiplying a vector \( x \) with \( A \) performs the Fourier transform. In this case, we are interested in the scenario where \( m = n \). We can also consider cases where \( A \) is a real random matrix or a complex random matrix, with \( m > n \) in the random scenarios.
\end{itemize}
In many problems in the world, as well as in our problem, we can assume certain things about our solution, which we refer to as additional information. This additional information consists of the constraints that our solution must meet.

In addition, we can define our problem in the following way, and the reason for doing so will become clearer later in the article (see~\ref{sec:set_projection_sudoku}). We can look for a point \( x \in \mathcal{A} \cap \mathcal{B} \), where \( \mathcal{B} \) is the set of all signals that satisfy Equation~\eqref{eq:model} and is defined as:
\begin{equation}
\mathcal{B} = \left\{ y \in \mathbb{C}^m : |y| = b \right\}
\label{eq:model2}
\end{equation}
The set \( \mathcal{A} \) typically represents additional constraints known about our solution, such as sparsity or finite support.



% \section{Historical Context and Applications}

% Phase retrieval has a significant history, especially in X-ray crystallography. As noted by Walther~\cite{walther1963synthesis}, in the past, scientists faced the challenge of determining the structure of crystalline materials using X-ray diffraction patterns, which provide only intensity information. This missing phase information, known as the "phase problem," was a major obstacle to accurately determining atomic structures. while the magnitude gives us the energy distribution, the phase reveals the actual structure. The phase retrieval problem extends beyond filling in missing data; it’s about unlocking the complete potential of our measurements to understand the true nature of the object being studied.

% Over the years, phase retrieval has been crucial in many fields. In optics, it’s used for designing systems and reconstructing images from diffraction patterns~\cite{born1999principles}. In astronomy, it helps improve image resolution by correcting for atmospheric distortions. With advances like X-ray free-electron lasers and electron microscopy, phase retrieval remains vital.

\section{Challenges and Limitations in Phase Retrieval Algorithms}
Over the past decade, considerable scholarly focus has been directed toward the computational and theoretical dimensions of the phase retrieval problem. To streamline the intricacies of mathematical analysis, a trend emerged wherein researchers adopted an idealized model—one abstracted from practical applications—where the entries of \(A\) are independently and identically distributed (i.i.d.) according to a normal distribution or other similar statistical frameworks. We shall henceforth designate the endeavor of reconstructing a signal under these conditions as the *random phase retrieval problem*. Among the most prevalent algorithms for addressing this problem are those that rely on the minimization of non-convex loss functions, such as non-convex least squares, through the application of first-order gradient methods (e.g., see  \cite{7029630, chen2016solvingrandomquadraticsystems, cai2015optimalratesconvergencenoisy, 8049465}). Importantly, this body of work has yielded robust theoretical assurances, demonstrating that the non-convex nature of the problem is typically well-behaved when the number of measurements \(m\) significantly exceeds the dimensionality \(n\) of the signal being recovered \cite{Sun_2017, Chen_2019}.

Regrettably, it has become evident that the random phase retrieval problem presents significantly less complexity compared to the authentic phase retrieval problem, particularly when \(A\) is a matrix of the Fourier-type. For most practical phase retrieval applications, algorithms developed for the random phase retrieval framework prove inadequate: the inherent non-convexity of the problem does not exhibit benign behavior, and gradient-based methods often converge to local minima, far from any global solution (see the detailed analysis in \cite{elser2018benchmarkproblemsphaseretrieval}). Consequently, despite extensive contributions to this literature, these methods have wielded only marginal impact on real-world applications. In practice, heuristic algorithms dominate, including the hybrid input-output (HIO) \cite{fienup1982phase}, difference map \cite{Elser:03}, relaxed averaged alternating reflections \cite{luke2004relaxed,Luke_2004}, and relaxed reflect reflect (RRR) \cite{elser2017matrix}. Each of these methods may be viewed as an extension of the Douglas-Rachford algorithm \cite{douglas1956numerical}; an introduction to these algorithms is provided in Chapter~\ref{chap:algorithm_iterative_steps}. By employing a slight terminological generalization, we shall refer to these approaches as Douglas-Rachford type algorithms. While
these algorithms demonstrate strong empirical performance, their properties in the context of the non-convex phase retrieval problem remain largely unexplored.

Several key challenges continue to hinder theoretical advancements in this domain, including:
\begin{itemize}
    \item \textbf{Non-Uniqueness:} Different signals can share the same Fourier magnitude, leading to ambiguity ~\cite{Candes_2015}.
    \item \textbf{Noise Sensitivity:} Noise in measurements can significantly affect the reconstruction quality~\cite{elser2018benchmarkproblemsphaseretrieval}.
    \item \textbf{Computational Complexity:} Solving phase retrieval problems often requires complex, computationally demanding algorithms~\cite{Li_2017}.
     \item \textbf{Non-Convexity:} The solution space is non-convex, making optimization methods prone to local minima~\cite{Sun_2017,Chen_2019}. Both the alternating projection technique and gradient-based methods often struggle to yield meaningful solutions; they tend to converge rapidly to suboptimal local minima, rather than reaching a solution point. In practice, a set of algorithms that can be viewed as extensions of the Douglas-Rachford scheme is commonly used.
\end{itemize}



% \section{Intuitive Understanding and Challenges in Phase Retrieval}

% The phase retrieval problem can be understood intuitively as follows: Given the magnitude of the Fourier transform of a signal, how do we recover the signal? The challenge arises because the Fourier phase is lost during measurement. Thus, reconstructing the signal involves dealing with incomplete information.

% % Key challenges include:
% % \begin{itemize}
% %     \item \textbf{Non-Uniqueness:} Different signals can share the same Fourier magnitude, leading to ambiguity ~\cite{Candes_2015}.
% %     \item \textbf{Noise Sensitivity:} Noise in measurements can significantly affect the reconstruction quality~\cite{elser2018benchmarkproblemsphaseretrieval}.
% %     \item \textbf{Computational Complexity:} Solving phase retrieval problems often requires complex, computationally demanding algorithms~\cite{Li_2017}.
% %      \item \textbf{Non-Convexity:} The solution space is non-convex, making optimization methods prone to local minima~\cite{Sun_2017,Chen_2019}. Both the alternating projection technique and gradient-based methods often struggle to yield meaningful solutions; they tend to converge rapidly to suboptimal local minima, rather than reaching a solution point. In practice, a set of algorithms that can be viewed as extensions of the Douglas-Rachford scheme is commonly used.
% % \end{itemize}
\section{Matrix Completion}
Similar to phase retrieval, matrix completion also aims to reconstruct data from incomplete information but faces unique challenges. Phase retrieval recovers phase from magnitude data, while matrix completion fills in missing entries.The algorithms we will discuss are designed to address both problems.
One of the most common examples for matrix completion is the Sudoku puzzle shown in Figure \ref{fig:sudokuExample}.

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures1/sudoku_1}
    \caption{Original Sudoku puzzle}
    \label{fig:sudokuOriginal}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures1/sudoku_2}
    \caption{Elements to be reconstructed (in blue)}
    \label{fig:sudokuMissing}
  \end{subfigure}
  \caption[An example of a matrix-completion problem is illustrated in Sudoku.]{An example of a matrix completion problem is the Sudoku game. We are given the initial clues (the elements we know) and need to reconstruct the missing elements (the cells painted in blue).}
  \label{fig:sudokuExample}
\end{figure}


Matrix completion has broad applications across different fields:

\begin{itemize}
    \item \textbf{Recommender Systems:} A well-known application of matrix completion is in collaborative filtering for recommender systems, where users typically rate only a small fraction of available items. The missing ratings can be inferred based on the observed data and used to recommend items users are likely to enjoy (e.g., products, movies - see Netflix Prize Model in~\ref{table:Netflix_table}) \cite{candes2008exactmatrixcompletionconvex}.

    \item \textbf{Image Processing:} In image inpainting, matrix completion helps to recover missing or corrupted pixels by treating the image as a matrix. Often, images exhibit a low-rank structure, which allows for effective restoration by completing the missing matrix entries based on this low-rank property \cite{JMLR:v11:mazumder10a}.

    \item \textbf{System Identification:} In fields like control systems, missing data due to sensor faults or communication errors can be problematic. Matrix completion can reconstruct incomplete measurement data, enabling more reliable system identification and control \cite{keshavan2009matrixcompletionentries}.
\end{itemize}


\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
 & \textbf{The Lion King} & \textbf{Avatar} & \textbf{Inception} & \textbf{Titanic} & \textbf{The Avengers} \\
\hline
\textbf{John} & 99\% & 85\% & 80\% & - & 75\% \\
\hline
\textbf{Emily} & - & 10\% & - & 5\% & 77\% \\
\hline
\textbf{Michael} & 99\% & 87\% & 12\% & 90\% & 80\% \\
\hline
\textbf{Sarah} & 91\% & - & 1\% & 97\% & - \\
\hline
\end{tabular}
\caption[Netflix Prize Model]{The Netflix Matrix Model, inspired by the Netflix Prize competition, predicts missing elements in a user-movie matrix based on algorithms. The Netflix Prize aimed to enhance movie recommendations by predicting user ratings from partial data. Key details, including data structure, clustering methods, and \textbf{matrix rank}, ensure the feasibility of this approach.}
\label{table:Netflix_table}
\end{table}


The foundational approach to matrix completion relies on low-rank matrix recovery. This method assumes that the complete matrix \( M \in \mathbb{R}^{m \times n} \) has a low rank \( r \), where \( r \ll \min(m, n) \). This low-rank condition implies that the matrix has inherent redundancy, allowing the observed data to be effectively represented with fewer components than the total number of entries.

Key Reasons for Using Low-Rank Matrix Completion:

\begin{itemize}
    \item \textbf{Data Reduction and Compact Representation:} Low-rank matrices allow for a compact representation of data, which reduces the number of parameters required to describe it. This property is crucial in matrix completion, as it implies that the observed entries contain enough information to reconstruct the entire matrix, even if many entries are missing.

    \item \textbf{Patterns and Redundancy:} Many types of real-world data exhibit low-rank structures, making low-rank assumptions both practical and meaningful. For instance, in recommender systems, users' preferences often cluster, resulting in a user-item matrix that is typically low-rank. Similarly, in images, spatial coherence ensures that neighboring pixels are highly correlated. This correlation creates redundancy, making images particularly well-suited for low-rank approximations in matrix completion tasks.


    \item \textbf{Theoretical Guarantees for Recovery:} With sufficient samples and under specific conditions (such as incoherence and random sampling), low-rank matrix completion can recover the missing entries with high probability \cite{candes2008exactmatrixcompletionconvex}. This means that, theoretically, a low-rank matrix can be exactly reconstructed from a subset of its entries, which is not generally possible for higher-rank matrices.

    \item \textbf{Computational Efficiency:} Exploiting the low-rank structure leads to efficient algorithms for matrix completion. For instance, nuclear norm minimization—a convex relaxation of rank minimization—enables the use of efficient optimization techniques that make matrix completion feasible for large-scale data \cite{JMLR:v11:mazumder10a}.
\end{itemize}

\section{Research Scope and Objectives}
This thesis aims to evaluate the performance of existing algorithms across different scenarios, highlighting their effectiveness and limitations in various contexts. The analysis focuses on three main cases:

\begin{enumerate}
    \item The matrix \( A \) in the model defined in \(~\eqref{eq:model}\) is a random matrix, where every element is drawn i.i.d. from a normal distribution, and it can be either real or complex. This case is called the "Random case". It is interesting, even though it is considered easier, because many researchers start with this case before addressing the next, more complex scenarios.

    \item The Original Phase Retrieval Problem: Using Fourier magnitude samples from a sensor to recover the missing phase information. In this case, our matrix \( A \) is the DFT matrix, and thus we refer to this case as the "DFT case." As explained in \cite{elser2018benchmarkproblemsphaseretrieval}, we can assume that the signal is sparse, which will be one of the constraints on our signal (referred to as a projection later on).

    \item Matrix completion: In this case, one constraint ensures that the initial elements are imposed, and the second constraint typically ensures that the rank of the matrix is \(r\). This scenario is referred to as "Matrix Completion".
\end{enumerate}

\chapter{Projection on Sets Method and Algorithms}
\label{chap:projectionSetsMethod}


\section{Detailed Projections for Case Studies}
In this section, we provide a comprehensive overview of the specific projection operations employed for each scenario analyzed in this paper, outlining how these projections are applied across various cases to solve the phase retrieval and matrix completion problems. As we have mentioned numerous times, the solution to our problem is to find the intersection point  \( x_0 \in \mathcal{A} \cap \mathcal{B} \). To address this, we define projectors onto these sets, with the requirement that for practical algorithms, these projectors must be efficiently computable.



For a general \( x \in \mathbb{C}^n \), let \( y = Ax \in \mathbb{C}^m \). We focus on projectors in terms of \( y \) rather than \( x \) because computing the projector onto \( \mathcal{B} \) is significantly less expensive \cite{Li_2017, levin2020notedouglasrachfordgradientsphase}. In the random and the DFT cases, the projector of \( y \) onto the set \( \mathcal{B} \) is defined as

\[
P_{\mathcal{B}}(y) = b \odot \text{phase}(y),
\]

where \( b \) represents the measured magnitudes (see equation ~\eqref{eq:model}), \( \odot \) denotes the point-wise product, and the phase operator is defined element-wise as

\[
\text{phase}(y)[i] := \frac{y[i]}{|y[i]|}, \quad y[i] \neq 0,
\]

and zero otherwise.


In the random case, the entries of the sensing matrix are typically drawn i.i.d. from a normal distribution with \( m > 2n \). A point \( y_0 \in \mathbb{C}^m \) that represents a solution should lie within the column space of the matrix \( A \), which means \( y_0 = A A^\dagger y_0 \), where \( A^\dagger \) denotes the pseudo-inverse of \( A \). The column space of \( A \), also known as the image of \( A \), includes all vectors that can be expressed as \( A x \) for some vector \( x \).

Thus, for any vector \( y \in \mathbb{C}^m \) to be a valid solution, it must be in the image of \( A \), meaning there exists a vector \( x \) such that \( y = A x \). Specifically, the projection of \( y \) onto the column space of \( A \) is given by:

\[
P_A(y) = A A^\dagger y.
\]

In the DFT case, we assume the additional information discussed earlier in the paper: the signal is known to be sparse. As a result, we impose a sparsity constraint on the solution (see \cite{elser2018benchmarkproblemsphaseretrieval}), in addition to the projection on B that we mentioned earlier.

Sparsity refers to the property where most elements of a vector are zero, with only a few non-zero entries. Specifically, for a vector \( x \in \mathbb{C}^n \), we say it is \( S \)-sparse if it has at most \( |S| \) non-zero entries. In this case, the second projection imposes the assumption of sparsity by retaining the \( |S| \) highest-magnitude elements of the vector and setting the rest to zero.

The projection operator \( P_S \) onto the space of \( S \)-sparse vectors is defined as follows: for a given vector \( x \in \mathbb{C}^n \), \( P_S(x) \) is obtained by:

\[
P_S(x)[i] =
\begin{cases}
x[i], & \text{if } i \text{ corresponds to one of the } |S| \text{ largest elements of } |x|, \\
0, & \text{otherwise}.
\end{cases}
\]

This projection ensures that the sparsity constraint is enforced on the signal, keeping the \( |S| \) largest-valued pixels unchanged while setting the rest to zero. By applying this projection iteratively, we can ensure that our solution aligns with the assumed sparsity of the signal.

\textbf{Note:} We impose the sparsity constraint on our vector \( x \) rather than on vector \( y \) as before. However, it is important to highlight that in the DFT case, the matrix \( A \) is invertible (IDFT). Therefore, it does not matter whether we apply the operations to \( y \) or \( x \), as long as we remain consistent in our approach.


As extensively discussed in the article by \cite{elser2018benchmarkproblemsphaseretrieval}, we will define the stopping conditions slightly differently here.
We set our stopping condition as follows:

\begin{equation}
\frac{\text{I}_\text{S}}{\text{I}_\text{F}} > 0.95
\label{eq:threshold_condition}
\end{equation}

where \(\text{I}_\text{F}\), the power of the entire image (the sum of each pixel squared), is defined as:

\[
\text{I}_\text{F} = \sum_{x=0}^{M-1} \sum_{y=0}^{M-1} \rho^2(x, y),
\]

and \(\text{I}_\text{S}\), the power of the pixels in the support \( S \), is defined as:

\[
\text{I}_\text{S} = \sum_{(x,y) \in S} \rho^2(x, y).
\]

where \( \rho(x, y) \) represents the values in the reconstructed image, and \( S \) represents the support of the image.


We note that increasing the threshold value above 0.95 will lead to longer convergence times, but the performance of the reconstruction will improve in terms of accuracy.


\section{Projections for Matrix Completion Case}

In the case of Matrix Completion, the projections are different from the previous cases. The first projection enforces the known entries of the matrix, ensuring that the matrix matches the given values at specific locations. This can be represented using a mask operator that preserves the known entries and leaves the unknown entries unchanged. The second projection imposes a rank constraint, forcing the matrix to have a rank of \( r \), which is assumed based on additional information.

Let \( M \in \mathbb{R}^{m \times n} \) represent the matrix we want to complete, and let \( \Omega \) denote the set of indices where the matrix entries are known. We define the mask operator \( P_{\Omega} \) as follows:

\[
P_{\Omega}(X,M)[i,j] =
\begin{cases}
M[i,j], & (i,j) \in \Omega, \\
X[i,j], & (i,j) \notin \Omega,
\end{cases}
\]
where \( X \) is the matrix before projection. This projection ensures that the given matrix elements remain unchanged, while the unknown elements can still be updated in subsequent iterations.

The second projection forces the matrix to have a rank of \( r \). This is done using the Singular Value Decomposition (SVD). Suppose the matrix \( M \) has the SVD:

\[
M = U \Sigma V^*
\]
where \( \Sigma \) is the diagonal matrix of singular values, and \( U \), \( V \) are orthogonal matrices. The projection onto the set of rank-\( r \) matrices, denoted as \( P_r(M) \), is given by:

\[
P_r(M) = U \Sigma_r V^*
\]

where \( \Sigma_r \) is obtained by retaining only the largest \( r \) singular values and setting the rest to zero. This projection ensures that the resulting matrix has rank \( r \), while keeping the structure of the matrix close to the original.

This process is analogous to frequency filtering: by imposing the rank-\( r \) constraint, we eliminate the smallest singular values (which can be seen as high-frequency components), but the overall structure of the matrix remains largely unchanged.


\section{Defining and Analyzing Key Algorithms for Phase Retrieval}
In the following sections, we will explore several key algorithms that are crucial for solving the phase retrieval problem. We will begin by presenting their mathematical definitions and formulations. Specifically, we will examine the Hybrid Input-Output (HIO), Relaxed Averaged Alternating Reflections (RAAR), and  Relaxed Reflect Reflect  (RRR) algorithms. After defining these algorithms, we will discuss the specific projections used in each scenario under investigation. These iterative algorithms, based on projection techniques, serve as generalizations of the Douglas-Rachford algorithm. They play a significant role in tackling the challenges of phase retrieval by iteratively refining solutions through projection-based updates. This discussion will provide deeper insights into their practical applications and effectiveness.

Let \( y \in \mathbb{C}^n \) be a vector in \( n \)-dimensional complex space. We define the projection of \( y \) onto a set \( \mathcal{A} \) as \( P_\mathcal{A}(y) \), and the projection onto a set \( \mathcal{B} \) as \( P_\mathcal{B}(y) \). We will explicitly state later what each projection is, and it also depends on the specific problem we are solving. A solution is a point where the projections onto both sets $\mathcal{A}$ and $\mathcal{B}$ coincide, meaning that the projections of the point are in the intersection $\mathcal{A} \cap \mathcal{B}$.

\begin{definition}
A point \( y_0 \in \mathbb{C}^m \) is considered a solution if it satisfies the condition \( P_{\mathcal{A}}(y_0) = P_{\mathcal{B}}(y_0) \).
\end{definition}


In practice, many times we will be satisfied with equality up to a point of tolerance, or it will be used as a requirement in the noisy case. This condition will constitute the stopping criterion for our algorithms (except where otherwise stated). Intuitively, when we refer to projections as constraints, it is clear that if the projection of the vector onto one space is the same as the projection onto another space, then this is a solution.





\section{Set Projection Example Illustrated with Sudoku}
\label{sec:set_projection_sudoku}

Before presenting a rigorous discussion on the projection methods, an illustrative example using Sudoku will be provided. One of the methods to solve Sudoku is with the help of the projection method and by viewing the problem visually. Let's say that our large space is the space of all solutions for arranging numbers 1-9 in the cells of the Sudoku matrix. To solve the game, we have to obey several rules. The first rule is that we must use the initial clues we received (i.e., the numbers we already have at the beginning of the game). The second rule requires us to have the numbers 1-9 in each column, as well as in each row and block. We will say that each rule is a constraint on our solution. Each constraint defines a subspace within our large space. To solve the Sudoku, we will want to find the intersection of all these subspaces, which means that all our constraints are satisfied.At this point, it is useful and clear to define our problem as outlined in ~\eqref{eq:model2}.
\begin{figure}[ht]
  \centering
  \resizebox{0.8\textwidth}{!}{ % Scale down the entire figure
    \begin{minipage}{\textwidth}
      \centering
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_1}
        \caption{Row constraint: Each row must contain the numbers 1-9.}
        \label{fig:sudokuRow}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_2}
        \caption{Column constraint: Each column must contain the numbers 1-9.}
        \label{fig:sudokuColumn}
      \end{subfigure}

      \vspace{0.5cm}

      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_3}
        \caption{Block constraint: Each of the 9 distinct 3x3 block must contain the numbers 1-9.}
        \label{fig:sudokuBlock}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth} % Further reduce width of each subfigure
        \centering
        \includegraphics[width=\textwidth]{figures1/Sudoku_constraint_4}
        \caption{Solution: The completed Sudoku grid where all constraints are satisfied.}
        \label{fig:sudokuSolution}
      \end{subfigure}
    \end{minipage}
  }
  \caption[Examples of Sudoku constraints]{Examples of Sudoku constraints: row, column, block, and the solution. Each constraint requires that the numbers 1-9 be used exactly once in the respective row, column, or block.}
  \label{fig:sudokuConstraints}
\end{figure}
After understanding how to define a problem using spaces and subspaces, we want to find the intersection point using a computationally efficient method. More mathematically, we start with a random initial vector in our large space. We then define a subspace according to the constraint and project the vector onto this subspace. This process ensures that our vector satisfies the specific constraint. Our problem arises when we have multiple constraints (see \ref{fig:sudokuConstraints}): if we force our solution to fulfill one constraint and then impose it on the second subspace, it will, of course, satisfy the second constraint but not necessarily fulfill the first.
The naive method in our case to solve the Sudoku is to iteratively project each constraint one by one and hope it converges. That is, to project one constraint, then the second constraint, and so on, and then again the first constraint, and so on again. Later, we will see smarter ways to solve this using others methods of projections.


\chapter{Algorithm Iterative Steps}
\label{chap:algorithm_iterative_steps}

In this chapter, we delve into the iterative mechanisms that underpin a selection of widely used algorithms. Starting from a random initial point in the solution space, each algorithm iteratively updates its estimate, leveraging mathematical operations designed to refine the solution progressively. The algorithms we will discuss include:

\begin{itemize}
    \item \textbf{Alternating Projections}
    \item \textbf{Hybrid Input-Output (HIO)}
    \item \textbf{Relaxed Averaged Alternating Reflections (RAAR)}
    \item \textbf{Relaxed Reflect Reflect (RRR)}
\end{itemize}

While the specifics of each algorithm will be detailed later, the unifying principles of \textit{projection}, \textit{reflection}, and \textit{relaxation} warrant a closer examination, as they form the foundation of the iterative steps employed by all the methods discussed.

\subsection*{Projection}
As explained in the previous chapter, projection refers to the operation of mapping a point onto a set, typically in such a way that the distance from the point to the set is minimized. In iterative algorithms, projection helps ensure that the solution remains consistent with a given constraint or set of constraints.

\subsection*{Reflection}
Reflection builds on the concept of projection by extending the operation. For a point \( x \) and a set \( S \), the reflection across \( S \) is given by
\[
R_S(x) = 2P_S(x) - x,
\]
where \( P_S(x) \) denotes the projection of \( x \) onto the set \( S \). Reflection is particularly useful in optimization and feasibility problems, as it can drive the solution away from invalid regions while amplifying the effect of constraints.

\subsection*{Relaxation}
Relaxation introduces a degree of flexibility to projection or reflection operations, allowing the algorithm to adjust the strength of these operations. For instance, in a relaxed projection, the operation may take the form
\[
x_{k+1} = x_k + \beta \big(P_S(x_k) - x_k\big),
\]
where \( \beta \) is a relaxation parameter that controls the step size or influence of the projection. Similarly, relaxation can be applied to reflections, offering a balance between aggressive and conservative updates to the solution. This flexibility is critical for managing convergence behavior, especially in challenging optimization landscapes.

By combining these fundamental operations with iterative strategies, the algorithms presented in this chapter achieve robust and efficient convergence. The mathematical formulation of these methods will clarify their distinctions and similarities, paving the way for a detailed comparison of their performances and applications.

\section{Alternating Projections(AP)}

The Alternating Projections method involves iteratively projecting onto different sets. The update step for this method is given by:
\begin{equation}
y^{(k+1)} = P_A \left( P_B \left( y^{(k)} \right) \right)
\end{equation}
where \( y^{(k)} \) is the vector at the \( k \)-th iteration, and \( P_A \) and \( P_B \) are the projection operators onto sets \( A \) and \( B \), respectively. This method alternates between projecting onto each set to iteratively approach a solution that satisfies all constraints. The main issue is that the process can sometimes become stuck at a specific value, causing all subsequent iterations to repeatedly produce the same result, as illustrated in Figure \ref{fig:algorithm_behaviors}.



\begin{figure}[h!]
    \centering

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_1}
        \caption{Start from a random point}
        \label{fig:proj_circle_1_1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_2}
        \caption{Projection on the first circle}
        \label{fig:proj_circle_1_2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_3}
        \caption{Projection on the second circle}
        \label{fig:proj_circle_1_3}
    \end{minipage}

    \vspace{0.5cm}

    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_4}
        \caption{Projection on the third circle}
        \label{fig:proj_circle_1_4}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_5}
        \caption{First step of the algorithm}
        \label{fig:proj_circle_1_5}
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/proj_circle_6}
        \caption{Convergence to the intersection point}
        \label{fig:proj_circle_1_6}
    \end{minipage}

    \caption[Illustration of an Alternating Projections algorithm using orthogonal projections.]{Illustration of an Alternating Projections algorithm. The figure shows iterations in a projection-based algorithm within a 2-dimensional space. The goal is to find the intersection point of three circles using orthogonal projections. The process starts from a random point and proceeds iteratively. These images are referenced from the lecture by Andrew Maiden in 2022 \cite{Maiden2022}.}
    \label{fig:proj_circle}
\end{figure}

\noindent In Figure \ref{fig:proj_circle}, an illustration of iterations in a projection-based algorithm in a 2-dimensional space is presented. In this example, the goal was to find the intersection of three circles using orthogonal projections onto them. The process began from a random point and continued iteratively until convergence to the solution. It is important to note that there are cases where convergence may be slow or may fail. This images is referenced from the lecture by Andrew Maiden in 2022 \cite{Maiden2022}.



\section{Hybrid Input-Output (HIO)}

The Hybrid Input-Output (HIO) algorithm updates its iterative step as follows:
\begin{equation}
y^{(k+1)} = y^{(k)} + P_A \left( (1 + \alpha)P_B(y^{(k)}) - y^{(k)} \right) - \alpha P_B \left( y^{(k)} \right)
\end{equation}
where \( \alpha \) is a parameter controlling the amount of feedback applied in the iteration process.

\section{Relaxed Averaged Alternating Reflections (RAAR)}

The Relaxed Averaged Alternating Reflections (RAAR) algorithm updates its iterative step as:
\begin{equation}
y^{(k+1)} = \beta   \left(y^{(k)}+ P_A \left( 2P_B(y^{(k)})- y^{(k)}\right) \right) +
(1-2\beta)  P_B \left( y^{(k)} \right)
\end{equation}

where \( \beta \) is a parameter controlling the balance between the two projections \( P_A \) and \( P_B \).

\section{Relaxed Reflect Reflect (RRR)}

The  Relaxed Reflect Reflect (RRR) algorithm updates its iterative step as:
\begin{equation}
y^{(k+1)} = y^{(k)} +\gamma  \left( P_A \left( 2P_B(y^{(k)})- y^{(k)}\right) -  P_B \left( y^{(k)} \right) \right)
\end{equation}

where \( \gamma \) is a parameter similar to \( \beta \) in RAAR, determining the weight given to each projection. One advantage of this algorithm is that it oscillates, which helps it avoid getting stuck in a local minimum, unlike the Alternating Projections (AP) method, see figure \ref{fig:algorithm_behaviors}. This oscillatory behavior often leads to excellent performance, as will be further discussed in the next chapter.
We will consider cases where \( \alpha \), \( \beta \), and \( \gamma \) lie in the interval \((0, 1)\).


\begin{figure}[h!]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/AP_example_stuck_1}
        \caption{Alternating Projections method getting stuck.}
        \label{fig:ap_stuck}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures1/RRR_example_oscillate_1}
        \caption{RRR algorithm exhibiting oscillations.}
        \label{fig:rrr_oscillate}
    \end{minipage}
    \caption[RRR and Alternating Projections algorithms exhibit different behaviors.]{Illustrations of different algorithm behaviors: (left) Alternating Projections method getting stuck and (right) RRR algorithm oscillating. This is an example of running a composite random matrix \(A\) drawn i.i.d. from a normal distribution with size \(50 \times 15\). The run was stopped after 1000 iterations when the Alternating Projections (AP) algorithm still did not converge at this stage, whereas the Relaxed Reflect Reflect (RRR) algorithm had already converged.
}
    \label{fig:algorithm_behaviors}
\end{figure}

\chapter{Theoretical Boundaries in Matrix Completion}

\section{Motivation: The Netflix Prize Model}

The Netflix Prize Model, as outlined in Table~\ref{table:Netflix_table}, organizes users as rows and movies as columns. The purpose of this model is to recommend movies to users based on their preferences, inferred from ratings they provide for certain movies. These ratings are typically represented as percentage values. A central question in this context is how many ratings need to be provided and how many can remain missing while still enabling accurate predictions. This challenge is closely tied to the matrix rank \(r\), which represents the inherent structure of the dataset.

The matrix rank offers a mathematical representation of the patterns in user preferences and movie characteristics. For example, similar users might show comparable rating tendencies, or individuals could exhibit preferences for specific genres. However, alternative mathematical formulations, such as explicit connections between genres or clusters of similar users, could also describe these relationships. While such approaches are interesting and potentially relevant, they fall beyond the scope of this discussion. Here, we focus solely on understanding and utilizing the rank of the matrix.

Our approach is motivated by the need to balance efficiency and effectiveness. Minimizing the effort required from users, such as avoiding unnecessary input requests, is crucial to creating a smooth user experience. At the same time, the system must maintain its ability to generate accurate recommendations. This balance highlights the fundamental trade-offs in matrix completion—between user interaction and algorithmic accuracy. For the purposes of this study, we concentrate exclusively on cases where the matrix rank is assumed to be known, and we explore how this assumption influences the theoretical boundaries of the problem.




\section{The Maximum Number of Elements that Can Be Deleted While Retaining Reconstruction}

\subsection*{Claim}
Let \( A \) be a square \( n \times n \) matrix of rank \( r \). It is possible to delete up to:
\[
q = n^2 - (2nr - r^2)
\]
elements while still ensuring \textbf{unique} reconstruction of the matrix, assuming only the rank of the matrix and the known elements are given.

\subsection*{Proof}

\begin{enumerate}
    \item \textbf{Rank \( r \):} A rank-\( r \) matrix can be expressed as the product of two smaller matrices: \( A = UV^\top \), where:
    \begin{itemize}
        \item \( U \) is an \( n \times r \) matrix, and
        \item \( V \) is an \( r \times n \) matrix.
    \end{itemize}
    The total number of parameters required to describe \( A \) is:
    \[
    \text{Total parameters} = nr + nr = 2nr.
    \]

    \item \textbf{Non-uniqueness of the decomposition:} The decomposition \( A = UV^\top \) is not unique because \( U \) can be multiplied by any invertible \( r \times r \) matrix, and \( V \) can be multiplied by its inverse. This introduces a redundancy of \( r^2 \) parameters. Thus, the number of \textit{unique} parameters needed to describe \( A \) is:
    \[
    \text{Unique parameters} = 2nr - r^2.
    \]

    \item \textbf{Minimum number of known elements:} To uniquely reconstruct the matrix, at least \( 2nr - r^2 \) entries of the matrix must be known.

    \item \textbf{Number of deletable elements:} Since the total number of elements in the matrix is \( n^2 \), the number of elements that can be deleted while still allowing for unique reconstruction is:
    \[
    q = n^2 - (2nr - r^2).
    \]
\end{enumerate}

\subsection*{Limitation of This Discussion}

This result assumes only two constraints:
\begin{enumerate}
    \item The rank \( r \) of the matrix is known.
    \item A sufficient number of matrix elements are known (i.e., \( n^2 - q \) elements remain).
\end{enumerate}
We do not consider cases where additional information or a third constraint is provided (e.g., sparsity, structural patterns, or specific relationships among matrix elements).

\subsection*{Testing Algorithm Stability Within Theoretical Bounds}
Based on this proof, the simulation in the Numerical Experiment section will test algorithm stability by varying the number of missing elements, without exceeding the theoretical bound established here.


\chapter{Numerical experiments}
In this chapter, I will present the results of my research, using graphs and simulations to clarify and illustrate the findings. I will refer to different cases to provide context and depth to the analysis, comparing various algorithms to highlight their performance in each scenario. Additionally, I will explain and interpret the data, offering insights and a detailed discussion supported by the simulations I built in the attached code.
% \newpage
\section{Random Case}

In this section, we describe the steps of our experiment and present the results and conclusions for the IID random case. Specifically, we generated a random IID matrix $\mathbf{A} \in \mathbb{C}^{m \times n}$ and a random vector $\mathbf{b} \in \mathbb{C}^n$. As defined in Section 111, the iterations are performed on the vector $\mathbf{y}$, which is computed as the product of $\mathbf{A}$ and $\mathbf{b}$, i.e., $\mathbf{y} = \mathbf{A} \mathbf{b}$.

In our system, we incorporate the magnitude of the product $\mathbf{y}$ along with the matrix $\mathbf{A}$, which is also provided. Note that our vectors and matrices are defined in the complex domain; however, the methodology can be applied to the smaller real-valued case.

The dimensions and parameters used in the experiment are:
\begin{itemize}
    \item $m = 25$
    \item $n = 8$
    \item $\beta = 0.5$, which we observed to be optimal in the vast majority of cases.
\end{itemize}

The vector $\mathbf{y}$ is initialized randomly, and we implement the following algorithms to solve the problem:
\begin{itemize}
    \item Alternating Projections
    \item RRR (Relaxed Reflect-Reflect)
    \item RAAR (Relaxed Averaged Alternating Reflections)
    \item HIO (Hybrid Input-Output)
\end{itemize}
We examine and analyze the performance of each algorithm under these conditions.
Our method demonstrates convergence when two iterations produce nearly identical outcomes. In this case, the convergence is achieved within an error margin of \(10^{-6}\).

We will present the convergence plots to visualize this behavior. Additionally, I will include a graphical representation that I find helpful for visualizing the results: showing a vector's values as a function of its index.
\begin{figure}[h!]
    \centering
    \includegraphics[width=1.1\textwidth]{phase_complex/vector_representation.png}
    \caption{Graphical representation of a vector as a function of its index.}
    \label{fig:vector_representation}
\end{figure}


After understanding the methodology, we can now present the results of our experiment and showcase the values of the vector as explained earlier. We present the results of our experiment, where we ran four algorithms, but only two of them converged. The RRR algorithm converged after 493 iterations, and the HIO algorithm converged after 726 iterations. We defined convergence as achieving the desired criteria within 100,000 iterations. If the algorithm did not converge within this limit, we considered it as not converged. In Figure~\ref{fig:results_m_25__n_10}, we illustrate the magnitude of our results alongside the original $b$ vector for comparison.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{phase_complex/m_25__n_10__beta_0_5__e_6_max_iter_100000_RRR_493_HIO_726.png}
    \caption{Convergence results of the algorithms. The magnitude of the results is shown alongside the original $b$ vector for comparison. Only RRR and HIO algorithms achieved convergence.}
    \label{fig:results_m_25__n_10}
\end{figure}

The convergence behavior of all four algorithms can be observed, with the solution defined by the projections onto the two sets being identical or nearly identical. To illustrate this, we present a graph showing the norm value between the projections as a function of the iteration count. If an algorithm does not converge within 10,000 iterations, it is classified as non-convergent. See Figure~\ref{fig:4_convergence_fig} for a visual representation of the convergence behavior of the algorithms.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/4_convergences/AP_C.png}
        \caption{AP Convergence}
        \label{fig:convergence_ap}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/4_convergences/RRR_C.png}
        \caption{RRR Convergence}
        \label{fig:convergence_rrr}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/4_convergences/HIO_C.png}
        \caption{HIO Convergence}
        \label{fig:convergence_hio}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/4_convergences/RAAR_C.png}
        \caption{RAAR Convergence}
        \label{fig:convergence_raar}
    \end{subfigure}
    \caption{Convergence of the four algorithms: AP, RRR, HIO, and RAAR.}
    \label{fig:4_convergence_fig}
\end{figure}


Now we present a statistical analysis combining data from 10,000 trials. The objective of this experiment is to evaluate the performance of four algorithms under the specific conditions where $m = 25$, $n = 8$, and $\beta = 0.5$. Each algorithm was tested on an identical dataset, and the aggregated results are shown in the figure below.

The figure below ~\ref{fig:percentage_results} illustrates the performance distribution of the four algorithms across the 10,000 trials. This includes the average success rates and the variability in performance.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{phase_complex/percentage.png}
    \caption{Performance distribution of 10,000 trials for the four algorithms, representing the success rates (convergence percentages) of each.}
    \label{fig:percentage_results}
\end{figure}

From the results, it is evident that the Alternating Projections and RAAR algorithms demonstrate similar levels of performance, achieving moderate success rates. These methods are consistent and reliable under the given experimental parameters but do not excel in particularly challenging scenarios. On the other hand, RRR and HIO show significantly better performance, with both achieving substantially higher success rates compared to the other two methods. This indicates that these algorithms are more robust and adaptable, likely benefiting from their unique iterative approaches. Among RRR and HIO, their performance differences are minor, but further analysis could reveal specific scenarios where one has a distinct advantage over the other. Overall, these results emphasize the strengths of each algorithm and provide a foundation for selecting the most suitable method depending on the problem context.

Beyond understanding the convergence rates, we are also interested in analyzing the number of iterations required for each algorithm to converge. To illustrate this, we include two images in Figure~\ref{fig:convergence_iterations_fig}. The first image shows the number of iterations required for each trial to converge for each algorithm, while non-convergent trials are omitted. The second image provides a zoomed-in view, making it easier to observe subtle differences in the convergence behavior across trials.

The results demonstrate a clear overlap between the convergence percentages and the number of iterations required for convergence. Specifically, the HIO algorithm converges slightly more frequently than the RRR algorithm, and both outperform the other two algorithms by a significant margin. It is important to note that while there are instances where the RRR algorithm required more iterations to converge compared to the AP or RAAR algorithms, there is no meaningful comparison between them. The RRR algorithm delivered outstanding performance overall, significantly exceeding the effectiveness of AP and RAAR.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/Converged_values.png}
        \caption{Iterations required for convergence for each algorithm (non-convergent trials omitted).}
        \label{fig:converged_value}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{phase_complex/zoom_in.png}
        \caption{Zoomed-in view of convergence behavior for better visualization.}
        \label{fig:zoom_in}
    \end{subfigure}
    \caption{Analysis of iterations required for convergence across all trials for each algorithm.}
    \label{fig:convergence_iterations_fig}
\end{figure}


\section{DFT Case}
In this scenario, we are dealing with a case close to reality where we assume our vector \( x \) is sparse, and we perform a Fourier transform as expanded in the previous discussion. The experimental setup here is similar to the previous one, but in this case, we emphasize that our vector \( x \) is sparse, as assumed. Specifically, \( x \) has \( S \) non-zero entries.

As mentioned earlier, our stopping condition is based on a threshold for the ratio, as defined in Equation ~\eqref{eq:threshold_condition}.

Below~\ref{fig:experiment_0_95}, we present the results for the experiment conducted using this threshold \( (0.95) \). The parameters for the experiment are \( n = 40 \), i.e., a \( 40 \times 40 \) Discrete Fourier Transform (DFT) matrix, and \( S = 4 \). Initially, the RRR (Relaxed Reflect-Reflect) algorithm is run to illustrate the method.



\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{dft_case/original/abs_fft_0_95}
    \hspace{0.05\textwidth}
    \includegraphics[width=0.40\textwidth]{dft_case/original/th_0_95}
    \caption{Results for threshold \( 0.95 \). Left: the reconstructed vector values. Right: convergence of the ratio as a function of iterations.}
    \label{fig:experiment_0_95}
\end{figure}

While this result is satisfactory, it is possible to achieve a better approximation of the original vector by increasing the threshold. For example, by raising the threshold to \( 0.999 \), we can achieve a result where the reconstructed vector is much closer to the original. However, this comes at the cost of requiring significantly more iterations for convergence, as shown in Figure~\ref{fig:experiment_0_999}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{dft_case/original/abs_fft_0_999}
    \hspace{0.05\textwidth}
    \includegraphics[width=0.4\textwidth]{dft_case/original/th_0_999}
    \caption{Results for threshold \( 0.999 \). Left: the reconstructed vector values, which are much closer to the original vector. Right: convergence of the ratio as a function of iterations, showing a slower rate.}
    \label{fig:experiment_0_999}
\end{figure}

We can also investigate the performance of the algorithm under noisy conditions. In our experiment (Figure~\ref{fig:convergence_iterations}), we added Gaussian noise with a mean of zero and a variance $\sigma^2$ to the input vector $x$. Specifically, we sampled 300 values for $\sigma$ in equal intervals ranging from $0$ to $10$.

The results show that as the noise level increases, it becomes increasingly difficult for the algorithm to converge. This is reflected in the number of iterations required and the density of points in the final stages. For high noise levels, the convergence nearly disappears, indicating that the algorithm struggles to settle as $\sigma$ approaches higher values. Points that did not converge are not visible on the graph.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{dft_case/original_noise/con_value_map}
    \caption{The number of iterations required for convergence as a function of $\sigma$ (noise variance). It is evident from the graph that higher noise levels lead to a significant increase in the iterations required for convergence.}
    \label{fig:convergence_iterations}
\end{figure}

From the data, the first $\sigma$ value for which the algorithm failed to converge was $2.27$. This is illustrated in Figure~\ref{fig:first_non_convergence}. Beyond this value, although some instances managed to converge, most failed to exhibit consistent convergence behavior. Additionally, the oscillatory nature of the algorithm can be observed in this graph, which explains why it performs well in many other cases despite the added noise.

It is worth noting that while the algorithm failed to converge at $\sigma = 2.27$ in this experiment (Figure~\ref{fig:first_non_convergence}), there were iterations where it reached values for which the ratio was high. In such iterations, even though the lack of precision might be significant, the results could still be considered sufficient, depending on the requirements of the task.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{dft_case/original_noise/first_non_con}
    \caption{The first noise variance ($\sigma = 2.27$) where the algorithm failed to converge. Oscillatory behavior is evident, showcasing the challenges introduced by high noise levels.}
    \label{fig:first_non_convergence}
\end{figure}

The results emphasize the impact of noise on the algorithm's convergence and highlight the challenges in achieving stability under noisy conditions.


After understanding how we perform the simulation, we aim to examine not just a single case but a large number of experiments to gain a broad statistical perspective. We run 10,000 experiments with a matrix size of $n=50$ (i.e., $50 \times 50$ matrices) and $S=5$. We analyze the convergence percentages for each algorithm (see Figure ~\ref{fig:percentage_dft}). Note that 10,000 is the maximum number of iterations considered for convergence; if an algorithm does not converge within this limit, it is treated as non-convergent.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{dft_case/generic/percentage}
    \caption{Convergence percentages for different algorithms over 10,000 experiments with matrix size $n=50$ and $S=5$ scenarios.}
    \label{fig:percentage_dft}
\end{figure}

Additionally, we also examine the convergence values, specifically how many iterations were required until convergence. Figure~\ref{fig:iterations} shows this analysis with two subplots. The first plot illustrates the iteration count until convergence as a function of the experiments. The second plot provides a zoomed-in view of the first plot, allowing a more detailed observation of the behavior. It is important to note that for experiments where the algorithm did not converge within 10,000 iterations, the corresponding data point is not shown in the plot.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{dft_case/generic/iter_count}
        \caption{Iteration count until convergence as a function of experiments.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{dft_case/generic/zoom_in}
        \caption{Zoomed-in view of the iteration count for better visibility.}
    \end{subfigure}
    \caption{Analysis of iteration counts required for convergence across experiments. For cases where the algorithm did not converge within 10,000 iterations, the data point is omitted.}
    \label{fig:iterations}
\end{figure}

We turn our attention to analyzing the results illustrated in the graph. The data reveals significant differences in performance across the algorithms. RRR stands out as the most robust, demonstrating consistently high convergence rates compared to the other methods. Its reliability across various scenarios highlights its effectiveness and versatility.

In terms of iteration count, RRR further distinguishes itself by requiring fewer iterations to converge compared to HIO. However, in cases where all algorithms successfully converge, both AP and RAAR occasionally achieve convergence in fewer iterations than RRR. Despite these exceptions, the overall performance gap remains substantial. RRR not only excels in its ability to converge but also does so efficiently, solidifying its position as the superior algorithm for the problem at hand.

% \newpage
\section{Matrix Completion}
I will begin by illustrating how the basic simulation was performed, using a simple example to lay the groundwork for understanding the methodology. Once the foundation is established, we will delve into more advanced concepts, exploring their intricacies and implications in greater detail.
As we know, our problem involves reconstructing a correct matrix, which I will refer to as the True matrix, using the non-missing elements provided, along with the additional information about the matrix's rank. This framework serves as the basis for our approach, guiding the reconstruction process to ensure accuracy and alignment with the given constraints.


In the example shown in Figures \ref{fig:example_matrices} and 3, you can see exactly how the simulation is performed. First, we created an $11 \times 11$ matrix with rank 3 and randomly selected 25 indices from the matrix to discard. We then provided two inputs to our code. The first input is the matrix we created after removing the 25 elements corresponding to the selected indices (this matrix is called the \textit{hint matrix}), and the second input is the information about the rank of the matrix, which in our case is 3.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/true.png}
        \caption{The true matrix (original matrix).}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/given.png}
        \caption{The hint matrix, along with the additional information that the matrix has a rank of 3, was provided.}
    \end{subfigure}
    \caption{Illustration of the original and hint matrices used in the simulation.}
    \label{fig:example_matrices}
\end{figure}
After receiving the inputs, we will begin solving our problem using the RRR algorithm to reconstruct the complete matrix. The process starts from a random initialization, as shown in Figure~\ref{fig:init_matrix}. For convenience, an element in the matrix will be colored green if it is similar to the value in the original matrix and red if it is different.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{Matrix_Completion/Example_of_running_2_matrices/init.png}
    \caption{The initial random matrix used as the starting point for the RRR algorithm.}
    \label{fig:init_matrix}
\end{figure}

In the following figure, we present the first iteration, the 33rd iteration, and the final iteration of the RRR algorithm. This visualization demonstrates the convergence process, where more elements of the matrix progressively approach the values of the original matrix, turning green. By the 33rd iteration, a significant number of elements are already close to the original values, highlighting the algorithm's progress. It is important to note that the graphs reflect the state of the matrix after enforcing the hint matrix projection. As a result, the red elements appear only in the missing entries, where there is no information from the hint matrix.

\begin{figure}[p]
    \centering
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/1th_iter.png}
        \caption{Iteration 1.}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/33th_iter.png}
        \caption{Iteration 33.}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Matrix_Completion/Example_of_running_2_matrices/end.png}
        \caption{Final iteration.}
    \end{subfigure}
    \caption[Visualization of the first, 33rd, and final iterations of the matrix in the RRR algorithm]{Visualization of the first, 33rd, and final iterations of the matrix in the RRR algorithm, showing the progression of convergence. Green elements indicate agreement with the original matrix, while red elements appear only in the missing entries after enforcing the hint matrix projection.}
    \label{fig:iterations}
\end{figure}


In addition to the visualization of matrix convergence, we evaluate the performance of the RRR algorithm graphically. Two metrics are plotted to provide insights into the algorithm's behavior across iterations. The first graph in Figure~\ref{fig:performance_plots} depicts the convergence of the algorithm by plotting the norm difference, \( |PB(y, b) - PA(y, A)| \), as a function of the iteration number. This graph is particularly significant, as convergence or a solution is defined when the projections are identical, or in our case, sufficiently similar within a tolerance of \(10^{-6}\).
The second graph in the same figure illustrates the norm difference between the reconstructed matrix and the original true matrix, \( |true\_matrix - iter\_matrix| \), highlighting the accuracy of the reconstruction over iterations. Together, these plots provide a comprehensive view of the convergence and effectiveness of the algorithm.

This example represents a relatively simple case, designed to demonstrate how the simulation operates. In the following sections, we will focus on more complex scenarios, where we are particularly interested in analyzing convergence or divergence, along with statistics and percentages derived from a larger set of experiments. These experiments will explore a broader range of parameter variations, such as matrix size, rank, and the proportion of missing entries.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=0.80\textwidth]{Matrix_Completion/Example_of_running_2_matrices/convergence.png}
        \caption{Projection error: \\ $|PB(y, b) - PA(y, A)|$ over iterations.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=0.80\textwidth]{Matrix_Completion/Example_of_running_2_matrices/convergence2.png}
        \caption{Reconstruction error: \\ $|true\_matrix - iter\_matrix|$ over iterations.}
    \end{subfigure}
    \caption[Convergence and reconstruction errors of the RRR algorithm]{Convergence and reconstruction errors of the RRR algorithm. The first graph shows the error between projections at each iteration, while the second shows how closely the reconstructed matrix approximates the original true matrix.}
    \label{fig:performance_plots}
\end{figure}



\subsection{Comparison of Matrix Completion Algorithms}

Now we investigate the performance of three matrix completion algorithms: \textbf{Alternating Projections}, \textbf{RRR}, and \textbf{RAAR}. The goal is to evaluate their ability to converge under a specific experimental setup. In this scenario, the matrix has a size of $n = 20$, a rank of $r = 3$, and $q = 50$ randomly missing elements. The convergence criterion requires that the solution must be achieved in no more than 1000 iterations. If the algorithm does not converge within this limit, it is deemed to have not converged. However, this threshold can be increased, similar to adjustments made in other sections.

To ensure a statistically robust analysis, the experiment was repeated $10,000$ times independently. The success rate of each algorithm—defined as the percentage of trials where convergence occurred—is summarized in Figure~\ref{fig:convergence}. These results provide valuable insights into the relative strengths of the algorithms under the given constraints.

From the results, it is evident that all three algorithms demonstrate high levels of reliability, with success rates consistently exceeding $97\%$. However, subtle differences between the algorithms offer meaningful insights. The \textbf{RRR} algorithm emerges as the most robust, achieving the highest convergence rate across all trials. This suggests that \textbf{RRR} is particularly well-suited for matrix completion tasks where a high level of precision and consistency is required.

The \textbf{RAAR} algorithm, while slightly less consistent than \textbf{RRR}, still performs remarkably well, demonstrating that it is a strong alternative in scenarios where computational constraints or other practical considerations might favor its use. Finally, \textbf{Alternating Projections}, despite its slightly lower success rate, provides a simpler approach that remains effective for most practical cases. This highlights a potential trade-off between algorithmic complexity and reliability.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Matrix_Completion/10000_trials/1}
    \caption[Convergence rates of the three algorithms over 10,000 trials.]{Convergence rates of the three algorithms over 10,000 trials. The graph shows the percentage of successful trials for each algorithm: Alternating Projections, RRR, and RAAR.}
    \label{fig:success_percentage}
\end{figure}

\subsection{Convergence Steps Analysis}

In addition to evaluating the overall success percentage, we also analyzed the number of iterations required for convergence in each trial. Figure~\ref{fig:iteration_analysis} provides a detailed view of this analysis. The graph on the left shows an overview of the iteration counts for all trials where convergence occurred, while the graph on the right zooms in to highlight specific trials and trends.

From these graphs, an important observation emerges: while the \textbf{RRR} algorithm achieved the highest overall convergence percentage, it does not necessarily converge the fastest. In cases where other algorithms also converged, they often did so in fewer iterations compared to \textbf{RRR}. This trade-off highlights a subtle difference in the behavior of these algorithms. For example, in the zoomed-in graph, it is evident that for a specific trial (e.g., Trial 8), no algorithm other than \textbf{RRR} managed to converge, further underscoring its robustness. However, the iteration counts in trials where convergence was achieved by multiple algorithms reveal that \textbf{Alternating Projections} and \textbf{RAAR} tend to require fewer steps, on average, to reach a solution.

It is important to note that cases where an algorithm did not converge are not represented in these graphs. Therefore, the percentage-based success results from Figure~\ref{fig:success_percentage} remain crucial for interpreting the overall effectiveness of the algorithms.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{Matrix_Completion/10000_trials/iteration_counts}
    \includegraphics[width=0.45\textwidth]{Matrix_Completion/10000_trials/iteration_zoom}
    \caption{(Left) Overview of iteration counts for all trials where convergence occurred. (Right) Zoomed-in view highlighting specific trials, such as Trial 8, where only \textbf{RRR} converged.}
    \label{fig:iteration_analysis}
\end{figure}

These findings emphasize the nuanced trade-offs between reliability and speed of convergence. While \textbf{RRR} remains the most robust algorithm, situations where fewer iterations are preferred might benefit from using \textbf{Alternating Projections} or \textbf{RAAR}, provided they meet the convergence requirements.

Overall, these findings illustrate that while all three algorithms are capable of addressing matrix completion problems, the choice of algorithm can be informed by the specific requirements of the task, such as computational efficiency or the need for maximal reliability. Future work could explore larger matrices, different rank configurations, or various levels of missing data to deepen our understanding of these algorithms' performance and scalability.

\subsection{Effect of Increasing Missing Elements}

Another important factor analyzed in this study is the effect of increasing the number of missing elements on the convergence behavior of the algorithms. In this experiment, we incrementally increase the number of missing elements $q$ from $1$ up to $(n - r)^2$, which represents the theoretical limit for matrix recovery as described in Theorem~\ref{thm:matrix_recovery_bound}. To ensure consistency across all trials, the maximum number of iterations was set to $\texttt{max\_iter} = 100,000$ for each algorithm. Two scenarios are considered:

\begin{itemize}
    \item \textbf{Case 1:} $n = 12$, $r = 3$
    \item \textbf{Case 2:} $n = 20$, $r = 5$
\end{itemize}

The results of these experiments are shown in Figure~\ref{fig:missing_elements_effect}. The first subplot represents the results for $n=12$ and $r=3$, while the second subplot represents $n=20$ and $r=5$. Each graph shows the number of iterations required for convergence as $q$ increases, but only for trials where the algorithms successfully converged. Trials where convergence did not occur (i.e., reached the maximum iteration limit without success) are excluded.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{Matrix_Completion/Increasing_Missing_Elements/n_12__r_3.png}
    \vspace{0.5cm}
    \includegraphics[width=0.8\textwidth]{Matrix_Completion/Increasing_Missing_Elements/n_20__r_5.png}
    \caption[Effect of increasing $q$ (number of missing elements) on convergence.]{Effect of increasing $q$ (number of missing elements) on convergence. (Top) $n=12$, $r=3$. (Bottom) $n=20$, $r=5$. Only trials where convergence occurred are shown. As $q$ approaches the theoretical limit $(n - r)^2$, \textbf{RRR} converges in more cases than the other algorithms. Trials reaching the maximum iteration limit of 100,000 are excluded, resulting in sparser points for algorithms that fail more often.}
    \label{fig:missing_elements_effect}
\end{figure}

From the figure, we observe a general upward trend in iteration counts for all algorithms, consistent with the expectation that increasing $q$ makes the problem more challenging. However, there is no dramatic difference in iteration counts among the algorithms in cases where they all converge. This indicates that \textbf{Alternating Projections}, \textbf{RRR}, and \textbf{RAAR} exhibit similar iteration growth patterns as the percentage of missing elements increases.

A critical observation is that \textbf{RRR} successfully converges in significantly more cases than the other algorithms, especially as $q$ approaches the theoretical limit $(n - r)^2$. This is reflected in the density of points on the graph: \textbf{RRR} shows a denser set of points, while the other algorithms exhibit sparser points, particularly at higher values of $q$, where they fail to converge in more trials. However, \textbf{RRR} does not necessarily converge faster in terms of iteration counts in trials where all algorithms succeed.

These results demonstrate that while all three algorithms show similar iteration trends for successful trials, \textbf{RRR}'s robustness allows it to converge in cases where the others fail, particularly as the problem becomes more difficult with a higher number of missing elements. The sparser points for \textbf{Alternating Projections} and \textbf{RAAR} at higher $q$ values reflect their inability to converge in many trials.

\chapter{Conclusion}


{
%\singlespacing
\backmatter
\bibliographystyle{unsrt}
\bibliography{references}
}

\end{document}







